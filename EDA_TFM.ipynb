{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3vlY48mvrI57"
   },
   "source": [
    "# 1. Web scraping from SENSACINE.COM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iyrW1solrI59"
   },
   "source": [
    "First, we need to import Beautiful Soup along with some other packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "GwFNNm-GrI59"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from dateutil.parser import parse\n",
    "import concurrent.futures\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xu-326WIrI5-"
   },
   "source": [
    "This application needs to download data from a large number of Sensacine URLs. We will be using Python’s concurrent API to make the process parallel and seamless."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "EJF_YDkQrI5-"
   },
   "outputs": [],
   "source": [
    "# Maximum number of threads that will be spawned\n",
    "MAX_THREADS = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QmChKakbrI5_"
   },
   "source": [
    "Attributes that we are interested in\n",
    "\n",
    "We will mostly focus on the below-mentioned attributes:\n",
    "\n",
    "    Movie title\n",
    "    Synopsis of the movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "VUo3xRXArI5_"
   },
   "outputs": [],
   "source": [
    "movie_title_arr = []\n",
    "movie_synopsis_arr =[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z0OYsxoWrI5_"
   },
   "source": [
    "Utility functions for Scraping above Data using Beautiful Soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "D6X4YS1lrI6A"
   },
   "outputs": [],
   "source": [
    "def getMovieTitle(title):\n",
    "    try:\n",
    "        return title[0].find(\"a\", {\"class\":  \"meta-title-link\"}).getText()\n",
    "    except:\n",
    "        return 'NA'\n",
    "\n",
    "def getsynopsis(synopsis):\n",
    "    try:\n",
    "        return synopsis[0].find(\"div\", {\"class\":  \"content-txt\"}).getText()\n",
    "    except:\n",
    "        return 'NA'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8vqICSvGrI6A"
   },
   "source": [
    "The main function that will utilize the URL provided to scrape data\n",
    "\n",
    "This will be our main function that will be responsible for iterating through the various attributes of the Sensacine data. We will be providing this function with URLs for various Sensacine pages and this will help us extract information from the pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "V-qSJk5_rI6A"
   },
   "outputs": [],
   "source": [
    "def main(sensacine_url):\n",
    "    response = requests.get(sensacine_url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Movie Name\n",
    "    movies_list  = soup.find_all(\"div\", {\"class\": \"card entity-card entity-card-list cf\"})\n",
    "    \n",
    "    # iterate over all movies\n",
    "    for movie in movies_list: \n",
    "        title = movie.find_all(\"h2\", {\"class\":  \"meta-title\"})\n",
    "        synopsis = movie.find_all(\"div\", {\"class\":  \"synopsis\"})\n",
    "        \n",
    "        #  Movie Title\n",
    "        movie_title =  getMovieTitle(title)\n",
    "        movie_title_arr.append(movie_title)\n",
    "        \n",
    "        # Movie Synopsys\n",
    "        movie_synopsis = getsynopsis(synopsis)\n",
    "        movie_synopsis_arr.append(movie_synopsis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "APUVoJ92rI6A"
   },
   "source": [
    "Note below mentioned for loop helps in generating URLs for the list of movies according to the filter that we have specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "H4uQgEMgrI6B"
   },
   "outputs": [],
   "source": [
    "# An array to store all the URL that are being queried\n",
    "url_arr = []\n",
    "\n",
    "# Maximum number of pages one wants to iterate over. \n",
    "# As we want to retrieve all the movies, we indicate the max number of pages possible\n",
    "MAX_PAGE = 3108\n",
    "\n",
    "# Loop to generate all the URLS.\n",
    "for i in range(0,MAX_PAGE):\n",
    "    totalRecords = 0 if i==0 else (250*i)+1\n",
    "    #print(totalRecords)\n",
    "    sensacine = f'https://www.sensacine.com/peliculas/todas-peliculas/'\n",
    "    url_arr.append(sensacine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UX6ez46lrI6B"
   },
   "source": [
    "The below-mentioned download function takes up the URLs and calls the main function with those. It does this in parallel with MAX_THREADS as the maximum number of requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "8htFZU1ArI6B"
   },
   "outputs": [],
   "source": [
    "def download_movies(movie_urls):\n",
    "    threads = min(MAX_THREADS, len(movie_urls))\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=threads) as executor:\n",
    "        executor.map(main, movie_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a2yCRvsCrI6C"
   },
   "source": [
    "Finally, we call the download function and then get our required data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "LlFAfrCnrI6C",
    "outputId": "a7db8e3e-d890-476a-c6f0-48bc8af52b3c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Synopsis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Spider-Man: No Way Home</td>\n",
       "      <td>\\nDespués de que Mysterio desvelara la identid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A través de mi ventana</td>\n",
       "      <td>\\nLa historia sigue a Raquel, una joven que ll...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Spider-Man: No Way Home</td>\n",
       "      <td>\\nDespués de que Mysterio desvelara la identid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Encanto</td>\n",
       "      <td>\\nEncanto nos sitúa en el corazón de Brasil, n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Uncharted</td>\n",
       "      <td>\\nEsta adaptación de la exitosa serie de video...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Title                                           Synopsis\n",
       "0  Spider-Man: No Way Home  \\nDespués de que Mysterio desvelara la identid...\n",
       "1   A través de mi ventana  \\nLa historia sigue a Raquel, una joven que ll...\n",
       "2  Spider-Man: No Way Home  \\nDespués de que Mysterio desvelara la identid...\n",
       "3                  Encanto  \\nEncanto nos sitúa en el corazón de Brasil, n...\n",
       "4                Uncharted  \\nEsta adaptación de la exitosa serie de video..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Call the download function with the array of URLS called url_arr\n",
    "download_movies(url_arr)\n",
    "\n",
    "# Attach all the data to the pandas dataframe\n",
    "movie_df = pd.DataFrame({\n",
    "    \"Title\": movie_title_arr,\n",
    "    \"Synopsis\": movie_synopsis_arr,\n",
    "})\n",
    "\n",
    "movie_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XEWM4QAzrI6D"
   },
   "source": [
    "There are *\\n*'s in the beggining of each synopsis string. We can get rid of it in order to clean the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 604
    },
    "id": "MXrh17cUrI6D",
    "outputId": "87a3c2d2-9187-4d71-e130-4075a84646bf",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Synopsis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Spider-Man: No Way Home</td>\n",
       "      <td>Después de que Mysterio desvelara la identidad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A través de mi ventana</td>\n",
       "      <td>La historia sigue a Raquel, una joven que llev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Spider-Man: No Way Home</td>\n",
       "      <td>Después de que Mysterio desvelara la identidad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Encanto</td>\n",
       "      <td>Encanto nos sitúa en el corazón de Brasil, nar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Uncharted</td>\n",
       "      <td>Esta adaptación de la exitosa serie de videoju...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46615</th>\n",
       "      <td>Eternals</td>\n",
       "      <td>Esta película basada en los cómics de Marvel n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46616</th>\n",
       "      <td>Cincuenta sombras de Grey</td>\n",
       "      <td>Anastasia Steele es una joven e inocente estud...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46617</th>\n",
       "      <td>Five Nights At Freddy's</td>\n",
       "      <td>Este proyecto que dirige Gil Kenan (Poltergeis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46618</th>\n",
       "      <td>La abuela</td>\n",
       "      <td>Susana tiene que dejar su vida en París trabaj...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46619</th>\n",
       "      <td>Harry Potter y la Piedra Filosofal</td>\n",
       "      <td>Harry Potter es un niño huérfano criado por su...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>46620 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Title  \\\n",
       "0                 Spider-Man: No Way Home   \n",
       "1                  A través de mi ventana   \n",
       "2                 Spider-Man: No Way Home   \n",
       "3                                 Encanto   \n",
       "4                               Uncharted   \n",
       "...                                   ...   \n",
       "46615                            Eternals   \n",
       "46616           Cincuenta sombras de Grey   \n",
       "46617             Five Nights At Freddy's   \n",
       "46618                           La abuela   \n",
       "46619  Harry Potter y la Piedra Filosofal   \n",
       "\n",
       "                                                Synopsis  \n",
       "0      Después de que Mysterio desvelara la identidad...  \n",
       "1      La historia sigue a Raquel, una joven que llev...  \n",
       "2      Después de que Mysterio desvelara la identidad...  \n",
       "3      Encanto nos sitúa en el corazón de Brasil, nar...  \n",
       "4      Esta adaptación de la exitosa serie de videoju...  \n",
       "...                                                  ...  \n",
       "46615  Esta película basada en los cómics de Marvel n...  \n",
       "46616  Anastasia Steele es una joven e inocente estud...  \n",
       "46617  Este proyecto que dirige Gil Kenan (Poltergeis...  \n",
       "46618  Susana tiene que dejar su vida en París trabaj...  \n",
       "46619  Harry Potter es un niño huérfano criado por su...  \n",
       "\n",
       "[46620 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_df = movie_df.replace(r'\\n','', regex=True)\n",
    "movie_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F4F5OT8trI6D"
   },
   "source": [
    "As we can see, we obtained description for 46620 movies which seems enough for the purpose of our project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HjOuA-kZrI6E"
   },
   "source": [
    "We can store our dataframe to a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "RqWtaeSQrI6E"
   },
   "outputs": [],
   "source": [
    "movie_df.to_csv('sensacine.csv', index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DGgkh2K73GrV"
   },
   "source": [
    "Exploratory Data Analysis for Natural Language Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Xvgkie-frI6E",
    "outputId": "a8553013-4c38-4759-d97c-7153a80ff680"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /Users/polina/opt/anaconda3/lib/python3.7/site-packages (1.2.4)\n",
      "Requirement already satisfied: matplotlib in /Users/polina/opt/anaconda3/lib/python3.7/site-packages (3.1.3)\n",
      "Requirement already satisfied: numpy in /Users/polina/opt/anaconda3/lib/python3.7/site-packages (1.20.0)\n",
      "Requirement already satisfied: nltk in /Users/polina/opt/anaconda3/lib/python3.7/site-packages (3.4.5)\n",
      "Requirement already satisfied: seaborn in /Users/polina/opt/anaconda3/lib/python3.7/site-packages (0.10.0)\n",
      "Requirement already satisfied: sklearn in /Users/polina/opt/anaconda3/lib/python3.7/site-packages (0.0)\n",
      "Requirement already satisfied: gensim in /Users/polina/opt/anaconda3/lib/python3.7/site-packages (3.8.0)\n",
      "Requirement already satisfied: pyldavis in /Users/polina/opt/anaconda3/lib/python3.7/site-packages (3.3.1)\n",
      "Collecting wordcloud\n",
      "  Using cached wordcloud-1.8.1.tar.gz (220 kB)\n",
      "Requirement already satisfied: textblob in /Users/polina/opt/anaconda3/lib/python3.7/site-packages (0.15.3)\n",
      "Requirement already satisfied: spacy in /Users/polina/opt/anaconda3/lib/python3.7/site-packages (2.1.8)\n",
      "Collecting textstat\n",
      "  Downloading textstat-0.7.2-py3-none-any.whl (101 kB)\n",
      "\u001b[K     |████████████████████████████████| 101 kB 1.9 MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.3 in /Users/polina/opt/anaconda3/lib/python3.7/site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in /Users/polina/opt/anaconda3/lib/python3.7/site-packages (from pandas) (2019.3)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /Users/polina/opt/anaconda3/lib/python3.7/site-packages (from matplotlib) (2.4.6)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/polina/opt/anaconda3/lib/python3.7/site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/polina/opt/anaconda3/lib/python3.7/site-packages (from matplotlib) (1.1.0)\n",
      "Requirement already satisfied: six in /Users/polina/opt/anaconda3/lib/python3.7/site-packages (from nltk) (1.15.0)\n",
      "Requirement already satisfied: scipy>=1.0.1 in /Users/polina/opt/anaconda3/lib/python3.7/site-packages (from seaborn) (1.4.1)\n",
      "Requirement already satisfied: scikit-learn in /Users/polina/opt/anaconda3/lib/python3.7/site-packages (from sklearn) (0.22.1)\n",
      "Requirement already satisfied: smart-open>=1.7.0 in /Users/polina/opt/anaconda3/lib/python3.7/site-packages (from gensim) (1.9.0)\n",
      "Requirement already satisfied: jinja2 in /Users/polina/opt/anaconda3/lib/python3.7/site-packages (from pyldavis) (2.11.1)\n",
      "Requirement already satisfied: joblib in /Users/polina/opt/anaconda3/lib/python3.7/site-packages (from pyldavis) (0.14.1)\n",
      "Requirement already satisfied: setuptools in /Users/polina/opt/anaconda3/lib/python3.7/site-packages (from pyldavis) (46.0.0.post20200309)\n",
      "Requirement already satisfied: future in /Users/polina/opt/anaconda3/lib/python3.7/site-packages (from pyldavis) (0.18.2)\n",
      "Requirement already satisfied: numexpr in /Users/polina/opt/anaconda3/lib/python3.7/site-packages (from pyldavis) (2.7.1)\n",
      "Requirement already satisfied: funcy in /Users/polina/opt/anaconda3/lib/python3.7/site-packages (from pyldavis) (1.16)\n",
      "Requirement already satisfied: pillow in /Users/polina/opt/anaconda3/lib/python3.7/site-packages (from wordcloud) (7.0.0)\n",
      "Requirement already satisfied: thinc<7.1.0,>=7.0.8 in /Users/polina/opt/anaconda3/lib/python3.7/site-packages (from spacy) (7.0.8)\n",
      "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /Users/polina/opt/anaconda3/lib/python3.7/site-packages (from spacy) (0.9.6)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/polina/opt/anaconda3/lib/python3.7/site-packages (from spacy) (2.0.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/polina/opt/anaconda3/lib/python3.7/site-packages (from spacy) (2.22.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/polina/opt/anaconda3/lib/python3.7/site-packages (from spacy) (1.0.0)\n",
      "Requirement already satisfied: srsly<1.1.0,>=0.0.6 in /Users/polina/opt/anaconda3/lib/python3.7/site-packages (from spacy) (0.1.0)\n",
      "Requirement already satisfied: blis<0.3.0,>=0.2.2 in /Users/polina/opt/anaconda3/lib/python3.7/site-packages (from spacy) (0.2.4)\n",
      "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /Users/polina/opt/anaconda3/lib/python3.7/site-packages (from spacy) (2.0.1)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.2.0 in /Users/polina/opt/anaconda3/lib/python3.7/site-packages (from spacy) (0.8.2)\n",
      "Requirement already satisfied: pyphen in /Users/polina/opt/anaconda3/lib/python3.7/site-packages (from textstat) (0.10.0)\n",
      "Requirement already satisfied: bz2file in /Users/polina/opt/anaconda3/lib/python3.7/site-packages (from smart-open>=1.7.0->gensim) (0.98)\n",
      "Requirement already satisfied: boto3 in /Users/polina/opt/anaconda3/lib/python3.7/site-packages (from smart-open>=1.7.0->gensim) (1.17.56)\n",
      "Requirement already satisfied: boto>=2.32 in /Users/polina/opt/anaconda3/lib/python3.7/site-packages (from smart-open>=1.7.0->gensim) (2.49.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /Users/polina/opt/anaconda3/lib/python3.7/site-packages (from jinja2->pyldavis) (1.1.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /Users/polina/opt/anaconda3/lib/python3.7/site-packages (from thinc<7.1.0,>=7.0.8->spacy) (4.42.1)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /Users/polina/opt/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /Users/polina/opt/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/polina/opt/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.25.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/polina/opt/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2019.11.28)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /Users/polina/opt/anaconda3/lib/python3.7/site-packages (from boto3->smart-open>=1.7.0->gensim) (0.10.0)\n",
      "Requirement already satisfied: botocore<1.21.0,>=1.20.56 in /Users/polina/opt/anaconda3/lib/python3.7/site-packages (from boto3->smart-open>=1.7.0->gensim) (1.20.56)\n",
      "Requirement already satisfied: s3transfer<0.5.0,>=0.4.0 in /Users/polina/opt/anaconda3/lib/python3.7/site-packages (from boto3->smart-open>=1.7.0->gensim) (0.4.2)\n",
      "Building wheels for collected packages: wordcloud\n",
      "  Building wheel for wordcloud (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for wordcloud: filename=wordcloud-1.8.1-cp37-cp37m-macosx_10_9_x86_64.whl size=158971 sha256=e0d5f7d4073e03f9d4ca47dc7e9c2d91597f6b1168c60ead9759c85f3c8e4dcb\n",
      "  Stored in directory: /Users/polina/Library/Caches/pip/wheels/f8/f6/55/6bd394c32a844a621ca0fe5dbf563c8d71d71edaf095656991\n",
      "Successfully built wordcloud\n",
      "Installing collected packages: wordcloud, textstat\n",
      "Successfully installed textstat-0.7.2 wordcloud-1.8.1\n"
     ]
    }
   ],
   "source": [
    "!pip install \\\n",
    "   pandas matplotlib numpy \\\n",
    "   nltk seaborn sklearn gensim pyldavis \\\n",
    "   wordcloud textblob spacy textstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359
    },
    "id": "lRGeahLp3ajR",
    "outputId": "caa6a64b-a6bb-4f8c-c343-2f979d181f62"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Synopsis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Spider-Man: No Way Home</td>\n",
       "      <td>Después de que Mysterio desvelara la identidad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A través de mi ventana</td>\n",
       "      <td>La historia sigue a Raquel, una joven que llev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Spider-Man: No Way Home</td>\n",
       "      <td>Después de que Mysterio desvelara la identidad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Encanto</td>\n",
       "      <td>Encanto nos sitúa en el corazón de Brasil, nar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Uncharted</td>\n",
       "      <td>Esta adaptación de la exitosa serie de videoju...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>El callejón de las almas perdidas</td>\n",
       "      <td>Remake de la cinta de 1947. Se desconoce el re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The Fallout</td>\n",
       "      <td>Vada es una estudiante de secundaria que no so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Moonfall</td>\n",
       "      <td>La Luna sale de su órbita y se dirige hacia la...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>My Hero Academia: Misión mundial de héroes</td>\n",
       "      <td>Una organización criminal conocida como Humani...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Cásate conmigo</td>\n",
       "      <td>Bastian (Maluma) y Kat Valdez (Jennifer López)...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Title  \\\n",
       "0                     Spider-Man: No Way Home   \n",
       "1                      A través de mi ventana   \n",
       "2                     Spider-Man: No Way Home   \n",
       "3                                     Encanto   \n",
       "4                                   Uncharted   \n",
       "5           El callejón de las almas perdidas   \n",
       "6                                 The Fallout   \n",
       "7                                    Moonfall   \n",
       "8  My Hero Academia: Misión mundial de héroes   \n",
       "9                              Cásate conmigo   \n",
       "\n",
       "                                            Synopsis  \n",
       "0  Después de que Mysterio desvelara la identidad...  \n",
       "1  La historia sigue a Raquel, una joven que llev...  \n",
       "2  Después de que Mysterio desvelara la identidad...  \n",
       "3  Encanto nos sitúa en el corazón de Brasil, nar...  \n",
       "4  Esta adaptación de la exitosa serie de videoju...  \n",
       "5  Remake de la cinta de 1947. Se desconoce el re...  \n",
       "6  Vada es una estudiante de secundaria que no so...  \n",
       "7  La Luna sale de su órbita y se dirige hacia la...  \n",
       "8  Una organización criminal conocida como Humani...  \n",
       "9  Bastian (Maluma) y Kat Valdez (Jennifer López)...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EWcGsA0EduRi",
    "outputId": "53c8065e-70a1-4aaf-b1c3-6cebf9d7f4a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: es_core_news_sm in /Users/polina/opt/anaconda3/lib/python3.7/site-packages (2.1.0)\n",
      "Requirement already satisfied: spacy>=2.1.0 in /Users/polina/opt/anaconda3/lib/python3.7/site-packages (from es_core_news_sm) (2.1.8)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/polina/opt/anaconda3/lib/python3.7/site-packages (from spacy>=2.1.0->es_core_news_sm) (2.22.0)\n",
      "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /Users/polina/opt/anaconda3/lib/python3.7/site-packages (from spacy>=2.1.0->es_core_news_sm) (2.0.1)\n",
      "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /Users/polina/opt/anaconda3/lib/python3.7/site-packages (from spacy>=2.1.0->es_core_news_sm) (0.9.6)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/polina/opt/anaconda3/lib/python3.7/site-packages (from spacy>=2.1.0->es_core_news_sm) (2.0.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/polina/opt/anaconda3/lib/python3.7/site-packages (from spacy>=2.1.0->es_core_news_sm) (1.0.0)\n",
      "Requirement already satisfied: srsly<1.1.0,>=0.0.6 in /Users/polina/opt/anaconda3/lib/python3.7/site-packages (from spacy>=2.1.0->es_core_news_sm) (0.1.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/polina/opt/anaconda3/lib/python3.7/site-packages (from spacy>=2.1.0->es_core_news_sm) (1.20.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.2.0 in /Users/polina/opt/anaconda3/lib/python3.7/site-packages (from spacy>=2.1.0->es_core_news_sm) (0.8.2)\n",
      "Requirement already satisfied: blis<0.3.0,>=0.2.2 in /Users/polina/opt/anaconda3/lib/python3.7/site-packages (from spacy>=2.1.0->es_core_news_sm) (0.2.4)\n",
      "Requirement already satisfied: thinc<7.1.0,>=7.0.8 in /Users/polina/opt/anaconda3/lib/python3.7/site-packages (from spacy>=2.1.0->es_core_news_sm) (7.0.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /Users/polina/opt/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.1.0->es_core_news_sm) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /Users/polina/opt/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.1.0->es_core_news_sm) (2.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/polina/opt/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.1.0->es_core_news_sm) (1.25.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/polina/opt/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.1.0->es_core_news_sm) (2019.11.28)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /Users/polina/opt/anaconda3/lib/python3.7/site-packages (from thinc<7.1.0,>=7.0.8->spacy>=2.1.0->es_core_news_sm) (4.42.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install es_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "33GLAn6OTnN7"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "def normaliza(texto):\n",
    "    #separamos después de ciertos signos de puntuación\n",
    "    texto = re.sub(r\"([\\.\\?])\", r\"\\1 \", texto)\n",
    "    doc = nlp(texto)\n",
    "    tokens = [t for t in doc if not t.is_punct and not t.is_stop and not t.is_space and len(t.text)>1]\n",
    "    palabras = []\n",
    "    for t in tokens:\n",
    "        if t.ent_iob_=='B' and t.ent_type_=='PER':\n",
    "            palabras.append('persona')\n",
    "        elif t.ent_iob_=='I' and t.ent_type_=='PER':\n",
    "            continue\n",
    "        else:\n",
    "            palabras.append(t.lemma_.lower()) \n",
    "    salida = ' '.join(palabras)\n",
    "    \n",
    "    return salida"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uWPEiDbXASjg"
   },
   "source": [
    "TOPIC MODELING.EDA, GENSIM, LDA model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k-SnLW054OU2"
   },
   "source": [
    "First of all, we install libraries and dependencies needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NSM1Iym14ZY4"
   },
   "source": [
    "For LDA model we would need to download a spacy model for a spanish languages as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9AiEDCIV4gLA",
    "outputId": "40b7cd8b-6449-4d25-c887-646405937ac7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /Users/polina/opt/anaconda3/lib/python3.7/site-packages (2.1.8)\n",
      "Requirement already satisfied: srsly<1.1.0,>=0.0.6 in /Users/polina/opt/anaconda3/lib/python3.7/site-packages (from spacy) (0.1.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/polina/opt/anaconda3/lib/python3.7/site-packages (from spacy) (1.20.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/polina/opt/anaconda3/lib/python3.7/site-packages (from spacy) (2.22.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/polina/opt/anaconda3/lib/python3.7/site-packages (from spacy) (1.0.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.2.0 in /Users/polina/opt/anaconda3/lib/python3.7/site-packages (from spacy) (0.8.2)\n",
      "Requirement already satisfied: blis<0.3.0,>=0.2.2 in /Users/polina/opt/anaconda3/lib/python3.7/site-packages (from spacy) (0.2.4)\n",
      "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /Users/polina/opt/anaconda3/lib/python3.7/site-packages (from spacy) (2.0.1)\n",
      "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /Users/polina/opt/anaconda3/lib/python3.7/site-packages (from spacy) (0.9.6)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/polina/opt/anaconda3/lib/python3.7/site-packages (from spacy) (2.0.2)\n",
      "Requirement already satisfied: thinc<7.1.0,>=7.0.8 in /Users/polina/opt/anaconda3/lib/python3.7/site-packages (from spacy) (7.0.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/polina/opt/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.25.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /Users/polina/opt/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/polina/opt/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2019.11.28)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /Users/polina/opt/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.8)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /Users/polina/opt/anaconda3/lib/python3.7/site-packages (from thinc<7.1.0,>=7.0.8->spacy) (4.42.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VyQf8rZf5Oyn",
    "outputId": "739fdf39-af47-4645-ad57-d41097fd0a0e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: es_core_news_md==2.1.0 from https://github.com/explosion/spacy-models/releases/download/es_core_news_md-2.1.0/es_core_news_md-2.1.0.tar.gz#egg=es_core_news_md==2.1.0 in /Users/polina/opt/anaconda3/lib/python3.7/site-packages (2.1.0)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('es_core_news_md')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download es_core_news_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "jXm5yzD34U64"
   },
   "outputs": [],
   "source": [
    "#nlp is now the variable that has the pre-trained spacy model\n",
    "import spacy\n",
    "nlp = spacy.load(\"es_core_news_md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jQFWVBND5s-a"
   },
   "source": [
    "Others:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "dJw33p1W5vQK"
   },
   "outputs": [],
   "source": [
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZbJU2zLu5rc1",
    "outputId": "d93744ee-c1cd-4003-85a5-7bc0f6a40851"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/polina/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## NLTK Libraries\n",
    "import nltk; \n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "VzWjDXki4NHg"
   },
   "outputs": [],
   "source": [
    "# Core Packages\n",
    "import os, re, operator, warnings\n",
    "warnings.filterwarnings('ignore')  # Let's not pay attention to them right now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H3AJaIiy58HN",
    "outputId": "7eeb2ba7-6804-4f2c-f3d5-45c518edb889"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/polina/opt/anaconda3/lib/python3.7/site-packages/sklearn/decomposition/_lda.py:29: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  EPS = np.finfo(np.float).eps\n"
     ]
    }
   ],
   "source": [
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dWeEeeaV59jr"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ITKkkkvJ6DmQ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UAUN2BC56EAs"
   },
   "source": [
    "Let's get a sample to work with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1dQ_TgAB6HQd",
    "outputId": "3e90c56d-7658-4ec0-ee7d-0dacfe39c3b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         Title  \\\n",
      "0                      Spider-Man: No Way Home   \n",
      "1                       A través de mi ventana   \n",
      "2                      Spider-Man: No Way Home   \n",
      "3                                      Encanto   \n",
      "4                                    Uncharted   \n",
      "5            El callejón de las almas perdidas   \n",
      "6                                  The Fallout   \n",
      "7                                     Moonfall   \n",
      "8   My Hero Academia: Misión mundial de héroes   \n",
      "9                               Cásate conmigo   \n",
      "10           Hotel Transilvania: Transformanía   \n",
      "11                      A través de mi ventana   \n",
      "12                                     Encanto   \n",
      "13                                   Uncharted   \n",
      "14           El callejón de las almas perdidas   \n",
      "\n",
      "                                             Synopsis  \n",
      "0   Después de que Mysterio desvelara la identidad...  \n",
      "1   La historia sigue a Raquel, una joven que llev...  \n",
      "2   Después de que Mysterio desvelara la identidad...  \n",
      "3   Encanto nos sitúa en el corazón de Brasil, nar...  \n",
      "4   Esta adaptación de la exitosa serie de videoju...  \n",
      "5   Remake de la cinta de 1947. Se desconoce el re...  \n",
      "6   Vada es una estudiante de secundaria que no so...  \n",
      "7   La Luna sale de su órbita y se dirige hacia la...  \n",
      "8   Una organización criminal conocida como Humani...  \n",
      "9   Bastian (Maluma) y Kat Valdez (Jennifer López)...  \n",
      "10  Regresan Drácula, su hija Mavis, su yerno huma...  \n",
      "11  La historia sigue a Raquel, una joven que llev...  \n",
      "12  Encanto nos sitúa en el corazón de Brasil, nar...  \n",
      "13  Esta adaptación de la exitosa serie de videoju...  \n",
      "14  Remake de la cinta de 1947. Se desconoce el re...  \n"
     ]
    }
   ],
   "source": [
    "sample = movie_df[:15]\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a5L3eM0c6Yci"
   },
   "source": [
    "**Approach by Understanding LDA**\n",
    "LDA stands for Latent Dirichlet Allocation, and it is a type of topic modeling algorithm. The purpose of LDA is to learn the representation of a fixed number of topics, and given this number of topics learn the topic distribution that each document in a collection of documents has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "tuwwzVSt6Uda"
   },
   "outputs": [],
   "source": [
    "## Stopwords from NTLK Libraries\n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('spanish') \n",
    "#This will import all the stopwords in Spanish language , similarly you can do for any other language.\"\"\"\n",
    "\n",
    "#stop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n",
    "#This attribute allows you to update your stop words list and make you pre-processing more aggressive and accurrate\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "id": "fpLQgGaH7HYb",
    "outputId": "15d6bf88-7936-475c-bde5-26af9404fc8c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'La historia sigue a Raquel, una joven que lleva casi toda la vida enamorada de su vecino Ares. Nunca le ha dicho lo que siente por él porque nunca han cruzado palabra alguna. Sin embargo, esto cambiará radicalmente cuando comiencen a surgir una serie de acontecimientos que les hará unirse cada vez más y más.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### To Iterate over each news in every line, we will have to convert it into a list\n",
    "\n",
    "data_list = sample['Synopsis'].tolist()\n",
    "data_list[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "id": "XCuUZlIG8G0B",
    "outputId": "a719ae4d-fe05-4c1c-ae43-8aec23d0aca2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'La historia sigue a Raquel  una joven que lleva casi toda la vida enamorada de su vecino Ares  Nunca le ha dicho lo que siente por él porque nunca han cruzado palabra alguna  Sin embargo  esto cambiará radicalmente cuando comiencen a surgir una serie de acontecimientos que les hará unirse cada vez más y más '"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_list  = [re.sub(r'[^\\w\\s]', ' ', line) for line in data_list]\n",
    "data_list[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "325O9GSo98kO"
   },
   "outputs": [],
   "source": [
    "#nlp is the model  that has the Spacy model pipleline built for English language \"\"\"\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"es_core_news_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4NFn__DG-HzU",
    "outputId": "c1949263-48d1-4ebe-f023-3b8e4b095143"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['la', 'que', 'el', 'en', 'y', 'a', 'los', 'del', 'se']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Define the stop words using NLTK corpus data\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words(\"spanish\")\n",
    "stop_words[1:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F662m7CS-UJu"
   },
   "source": [
    "Tokenize the text to convert them into words format from the sentence format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HbrbhMO1-PlC",
    "outputId": "2103e119-1556-4487-c532-c28b93b16488"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['despues', 'de', 'que', 'mysterio', 'desvelara', 'la', 'identidad', 'de', 'spider', 'man', 'todo', 'el', 'mundo', 'en', 'lejos', 'de', 'casa', 'peter', 'parker', 'tom', 'holland', 'desesperado', 'por', 'volver', 'la', 'normalidad', 'recuperar', 'su', 'anterior', 'vida', 'pide', 'ayuda', 'doctor', 'strange', 'para', 'enmendar', 'tal', 'accion', 'pero', 'alterar', 'la', 'realidad', 'tendra', 'consecuencias', 'nefastas', 'para', 'el', 'heroe', 'de', 'nueva', 'york']]\n"
     ]
    }
   ],
   "source": [
    "# Gensim’s simple_preprocess() is great for this. Additionally we have set deacc=True to remove the punctuations.\n",
    "\n",
    "def sentence_to_word(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "data_words = list(sentence_to_word(data_list))\n",
    "\n",
    "print(data_words[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1kcY1tv--dcs"
   },
   "source": [
    "**Bigrams and Trigrams creation using Gensim Phrase.**\n",
    "\n",
    "*Automatically detect common phrases – aka multi-word expressions, word n-gram collocations – from a stream of sentences.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OQ0bq-Y3-a46",
    "outputId": "eb277706-1141-4d09-cb60-aec52ee6d1a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['despues', 'de', 'que', 'mysterio', 'desvelara', 'la', 'identidad', 'de', 'spider', 'man', 'todo', 'el', 'mundo', 'en', 'lejos', 'de', 'casa', 'peter', 'parker', 'tom', 'holland', 'desesperado', 'por', 'volver', 'la', 'normalidad', 'recuperar', 'su', 'anterior', 'vida', 'pide', 'ayuda', 'doctor', 'strange', 'para', 'enmendar', 'tal', 'accion', 'pero', 'alterar', 'la', 'realidad', 'tendra', 'consecuencias', 'nefastas', 'para', 'el', 'heroe', 'de', 'nueva', 'york']\n"
     ]
    }
   ],
   "source": [
    "# Build the bigram and trigram models\n",
    "\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "# See trigram example\n",
    "print(trigram_mod[bigram_mod[data_words[0]]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "81wZDio6-0AK"
   },
   "source": [
    "Below is the utility functions used for basic preprocessing of Text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "vVvRKjSm-MFu"
   },
   "outputs": [],
   "source": [
    "# def remove_stopwords(text):\n",
    "#     for doc in text:\n",
    "#         if doc not in stop_words:\n",
    "#             return [words for words in simple_preprocess(str(doc))]\n",
    "                                    \n",
    "                    \n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qTYSBFXX-7AD",
    "outputId": "f932e9f3-9f7a-4876-e895-17321ddf766d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['despues', 'mysterio', 'desvelar', 'identidad', 'man', 'mundo', 'lejos', 'casar', 'peter', 'tom', 'holland', 'desesperar', 'volver', 'normalidad', 'recuperar', 'anterior', 'vida', 'pedir', 'ayudar', 'doctor', 'enmendar', 'accion', 'alterar', 'realidad', 'tendra', 'consecuencia', 'nefasto', 'heroe', 'nuevo', 'york']]\n"
     ]
    }
   ],
   "source": [
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "# python3 -m spacy download en_core_web_sm\n",
    "nlp = spacy.load('es_core_news_md', disable=['parser', 'ner'])\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "print(data_lemmatized[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a4BjwJ2S_Dlj"
   },
   "source": [
    "Data Transformation : - Dictionary and Corpus\n",
    "\n",
    "The two main inputs to the LDA topic model are the dictionary(id2word) and the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WexbLLM2_BRP",
    "outputId": "cb1845ec-4c8d-40ef-9cf0-f8f3d09a5781"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 1)]]\n"
     ]
    }
   ],
   "source": [
    "# Create Dictionary\n",
    "\n",
    "\"\"\"This module implements the concept of the Dictionary in Pytons, a mapping between words and their integer ids.\"\"\"\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_lemmatized\n",
    "\n",
    "# Term Document Frequency\n",
    "\"\"\"doc2bow us Document to Bag of Words format, It outputs a tuple of token id and token words\"\"\"\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "# View\n",
    "\n",
    "print(corpus[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KFY91FcW_Xzs"
   },
   "source": [
    "In bag of Words as you might know, that the model creates a corpus with assigning unique ID to individual word, and then providing the frequency of the word. This helps to define how important that word is in that particular document.\n",
    "\n",
    "For example, (0, 1) above implies, word id 0 occurs once in the first document. Likewise, word id 1 occurs twice and so on.\n",
    "\n",
    "This is used as the input by the LDA model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LRJ0dU4K_IoT",
    "outputId": "5c610b65-8a57-43e4-ebba-f78c187db111"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('accion', 1),\n",
       "  ('alterar', 1),\n",
       "  ('anterior', 1),\n",
       "  ('ayudar', 1),\n",
       "  ('casar', 1),\n",
       "  ('consecuencia', 1),\n",
       "  ('desesperar', 1),\n",
       "  ('despues', 1),\n",
       "  ('desvelar', 1),\n",
       "  ('doctor', 1),\n",
       "  ('enmendar', 1),\n",
       "  ('heroe', 1),\n",
       "  ('holland', 1),\n",
       "  ('identidad', 1),\n",
       "  ('lejos', 1),\n",
       "  ('man', 1),\n",
       "  ('mundo', 1),\n",
       "  ('mysterio', 1),\n",
       "  ('nefasto', 1),\n",
       "  ('normalidad', 1),\n",
       "  ('nuevo', 1),\n",
       "  ('pedir', 1),\n",
       "  ('peter', 1),\n",
       "  ('realidad', 1),\n",
       "  ('recuperar', 1),\n",
       "  ('tendra', 1),\n",
       "  ('tom', 1),\n",
       "  ('vida', 1),\n",
       "  ('volver', 1),\n",
       "  ('york', 1)]]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Human readable format of corpus (term-frequency)\n",
    "#This fucntion creates the reverse mapping of ID to original lemmatized word\"\"\"\n",
    "[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LZexWtla_k8g"
   },
   "source": [
    "Building the Topic Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9BgGJR3t_mcr"
   },
   "source": [
    "Refer this Genesims document for detailed information of LDA model and its hyperparameter for tuning purposes.\n",
    "https://radimrehurek.com/gensim/auto_examples/tutorials/run_lda.html#sphx-glr-auto-examples-tutorials-run-lda-py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DEGPh2qP_tnU"
   },
   "source": [
    "Important Parameters however which you must know:\n",
    "Number of Topics;- It can be anything as per output expectations, or the data itself. For our data we will take K = 20, as we know the data has 20 news groups.\n",
    "\n",
    "chuncksize: - Controls how many documents are processed at a time in the training algorithm.\n",
    "\n",
    "passes: - controls how often we train the model on the entire corpus, same like \" epochs\"\n",
    "\n",
    "We set alpha = 'auto' and eta = 'auto'. Again this is somewhat technical, but essentially we are automatically learning two parameters in the model that we usually would have to specify explicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ntO0faSe_sPR",
    "outputId": "76127ddb-be8a-4ecb-b368-459e949ca353"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-21 17:45:38,456 : INFO : using autotuned alpha, starting with [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]\n",
      "2022-02-21 17:45:38,457 : INFO : using symmetric eta at 0.05\n",
      "2022-02-21 17:45:38,459 : INFO : using serial LDA version on this node\n",
      "2022-02-21 17:45:38,461 : INFO : running online (multi-pass) LDA training, 20 topics, 10 passes over the supplied corpus of 15 documents, updating model once every 15 documents, evaluating perplexity every 15 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "2022-02-21 17:45:38,471 : INFO : -25.817 per-word bound, 59131536.4 perplexity estimate based on a held-out corpus of 15 documents with 384 words\n",
      "2022-02-21 17:45:38,471 : INFO : PROGRESS: pass 0, at document #15/15\n",
      "2022-02-21 17:45:38,481 : INFO : optimized alpha [0.044027206, 0.04006174, 0.04006174, 0.04728905, 0.043974794, 0.05174021, 0.04363027, 0.055702336, 0.043817613, 0.043595154, 0.041958645, 0.04006174, 0.04399643, 0.043897785, 0.0479096, 0.04399643, 0.047717955, 0.04006174, 0.04006174, 0.043856505]\n",
      "2022-02-21 17:45:38,484 : INFO : topic #17 (0.040): 0.004*\"orbitar\" + 0.004*\"oportunidad\" + 0.004*\"salvar\" + 0.004*\"salir\" + 0.004*\"provocar\" + 0.004*\"planeta\" + 0.004*\"tierra\" + 0.004*\"improbable\" + 0.004*\"junto\" + 0.004*\"unica\"\n",
      "2022-02-21 17:45:38,484 : INFO : topic #18 (0.040): 0.004*\"orbitar\" + 0.004*\"oportunidad\" + 0.004*\"salvar\" + 0.004*\"salir\" + 0.004*\"provocar\" + 0.004*\"planeta\" + 0.004*\"tierra\" + 0.004*\"improbable\" + 0.004*\"junto\" + 0.004*\"unica\"\n",
      "2022-02-21 17:45:38,484 : INFO : topic #14 (0.048): 0.032*\"seguidor\" + 0.032*\"bastian\" + 0.032*\"kat\" + 0.032*\"casarse\" + 0.020*\"cintar\" + 0.019*\"manipular\" + 0.017*\"lopez\" + 0.017*\"garden\" + 0.017*\"artista\" + 0.017*\"dia\"\n",
      "2022-02-21 17:45:38,485 : INFO : topic #5 (0.052): 0.044*\"mas\" + 0.032*\"cintar\" + 0.029*\"historia\" + 0.029*\"nunca\" + 0.027*\"manipular\" + 0.017*\"amante\" + 0.017*\"hombre\" + 0.017*\"incluso\" + 0.017*\"antojar\" + 0.017*\"desconocer\"\n",
      "2022-02-21 17:45:38,485 : INFO : topic #7 (0.056): 0.026*\"alterar\" + 0.026*\"mundo\" + 0.019*\"ayudar\" + 0.019*\"mysterio\" + 0.019*\"man\" + 0.019*\"nefasto\" + 0.019*\"pedir\" + 0.019*\"normalidad\" + 0.019*\"volver\" + 0.019*\"despues\"\n",
      "2022-02-21 17:45:38,486 : INFO : topic diff=15.984051, rho=1.000000\n",
      "2022-02-21 17:45:38,491 : INFO : -6.375 per-word bound, 83.0 perplexity estimate based on a held-out corpus of 15 documents with 384 words\n",
      "2022-02-21 17:45:38,492 : INFO : PROGRESS: pass 1, at document #15/15\n",
      "2022-02-21 17:45:38,495 : INFO : optimized alpha [0.041298166, 0.036161542, 0.036161542, 0.041859806, 0.041230805, 0.05003441, 0.039006364, 0.055962678, 0.03915403, 0.038978666, 0.037681356, 0.036161542, 0.043222383, 0.043046866, 0.04452012, 0.0412586, 0.046457503, 0.036161542, 0.036161542, 0.03918466]\n",
      "2022-02-21 17:45:38,496 : INFO : topic #18 (0.036): 0.004*\"orbitar\" + 0.004*\"oportunidad\" + 0.004*\"salvar\" + 0.004*\"salir\" + 0.004*\"provocar\" + 0.004*\"planeta\" + 0.004*\"tierra\" + 0.004*\"improbable\" + 0.004*\"junto\" + 0.004*\"unica\"\n",
      "2022-02-21 17:45:38,496 : INFO : topic #2 (0.036): 0.004*\"orbitar\" + 0.004*\"oportunidad\" + 0.004*\"salvar\" + 0.004*\"salir\" + 0.004*\"provocar\" + 0.004*\"planeta\" + 0.004*\"tierra\" + 0.004*\"improbable\" + 0.004*\"junto\" + 0.004*\"unica\"\n",
      "2022-02-21 17:45:38,496 : INFO : topic #16 (0.046): 0.040*\"joven\" + 0.040*\"seriar\" + 0.039*\"sullivan\" + 0.039*\"mentor\" + 0.039*\"mark\" + 0.039*\"drake\" + 0.039*\"crear\" + 0.039*\"descubrir\" + 0.039*\"adaptacion\" + 0.039*\"victor\"\n",
      "2022-02-21 17:45:38,497 : INFO : topic #5 (0.050): 0.043*\"cintar\" + 0.041*\"manipular\" + 0.033*\"mas\" + 0.027*\"historia\" + 0.022*\"amante\" + 0.022*\"hombre\" + 0.022*\"incluso\" + 0.022*\"antojar\" + 0.022*\"desconocer\" + 0.022*\"asi\"\n",
      "2022-02-21 17:45:38,497 : INFO : topic #7 (0.056): 0.028*\"alterar\" + 0.028*\"mundo\" + 0.020*\"ayudar\" + 0.020*\"mysterio\" + 0.020*\"man\" + 0.020*\"nefasto\" + 0.020*\"pedir\" + 0.020*\"normalidad\" + 0.020*\"volver\" + 0.020*\"despues\"\n",
      "2022-02-21 17:45:38,497 : INFO : topic diff=0.250049, rho=0.577350\n",
      "2022-02-21 17:45:38,501 : INFO : -5.976 per-word bound, 62.9 perplexity estimate based on a held-out corpus of 15 documents with 384 words\n",
      "2022-02-21 17:45:38,501 : INFO : PROGRESS: pass 2, at document #15/15\n",
      "2022-02-21 17:45:38,504 : INFO : optimized alpha [0.039201252, 0.033330016, 0.033330016, 0.03806814, 0.039125104, 0.048613, 0.03571287, 0.055977155, 0.03583561, 0.035689835, 0.03460734, 0.033330016, 0.042514324, 0.042282965, 0.041970715, 0.039156515, 0.045375437, 0.033330016, 0.033330016, 0.035861056]\n",
      "2022-02-21 17:45:38,504 : INFO : topic #2 (0.033): 0.004*\"orbitar\" + 0.004*\"oportunidad\" + 0.004*\"salvar\" + 0.004*\"salir\" + 0.004*\"provocar\" + 0.004*\"planeta\" + 0.004*\"tierra\" + 0.004*\"improbable\" + 0.004*\"junto\" + 0.004*\"unica\"\n",
      "2022-02-21 17:45:38,505 : INFO : topic #17 (0.033): 0.004*\"orbitar\" + 0.004*\"oportunidad\" + 0.004*\"salvar\" + 0.004*\"salir\" + 0.004*\"provocar\" + 0.004*\"planeta\" + 0.004*\"tierra\" + 0.004*\"improbable\" + 0.004*\"junto\" + 0.004*\"unica\"\n",
      "2022-02-21 17:45:38,505 : INFO : topic #16 (0.045): 0.040*\"joven\" + 0.040*\"seriar\" + 0.040*\"sullivan\" + 0.039*\"mentor\" + 0.039*\"mark\" + 0.039*\"drake\" + 0.039*\"crear\" + 0.039*\"descubrir\" + 0.039*\"adaptacion\" + 0.039*\"victor\"\n",
      "2022-02-21 17:45:38,505 : INFO : topic #5 (0.049): 0.047*\"cintar\" + 0.046*\"manipular\" + 0.029*\"mas\" + 0.027*\"historia\" + 0.024*\"amante\" + 0.024*\"hombre\" + 0.024*\"incluso\" + 0.024*\"antojar\" + 0.024*\"desconocer\" + 0.024*\"asi\"\n",
      "2022-02-21 17:45:38,506 : INFO : topic #7 (0.056): 0.029*\"alterar\" + 0.029*\"mundo\" + 0.020*\"ayudar\" + 0.020*\"mysterio\" + 0.020*\"man\" + 0.020*\"nefasto\" + 0.020*\"pedir\" + 0.020*\"normalidad\" + 0.020*\"volver\" + 0.020*\"despues\"\n",
      "2022-02-21 17:45:38,506 : INFO : topic diff=0.173617, rho=0.500000\n",
      "2022-02-21 17:45:38,510 : INFO : -5.788 per-word bound, 55.3 perplexity estimate based on a held-out corpus of 15 documents with 384 words\n",
      "2022-02-21 17:45:38,511 : INFO : PROGRESS: pass 3, at document #15/15\n",
      "2022-02-21 17:45:38,513 : INFO : optimized alpha [0.03750205, 0.031128522, 0.031128522, 0.03519772, 0.037420366, 0.0473902, 0.033185977, 0.05585006, 0.033291347, 0.033166196, 0.03223419, 0.031128522, 0.041862875, 0.041589092, 0.039936963, 0.037454057, 0.044423733, 0.031128522, 0.031128522, 0.033313192]\n",
      "2022-02-21 17:45:38,513 : INFO : topic #17 (0.031): 0.004*\"orbitar\" + 0.004*\"oportunidad\" + 0.004*\"salvar\" + 0.004*\"salir\" + 0.004*\"provocar\" + 0.004*\"planeta\" + 0.004*\"tierra\" + 0.004*\"improbable\" + 0.004*\"junto\" + 0.004*\"unica\"\n",
      "2022-02-21 17:45:38,514 : INFO : topic #1 (0.031): 0.004*\"orbitar\" + 0.004*\"oportunidad\" + 0.004*\"salvar\" + 0.004*\"salir\" + 0.004*\"provocar\" + 0.004*\"planeta\" + 0.004*\"tierra\" + 0.004*\"improbable\" + 0.004*\"junto\" + 0.004*\"unica\"\n",
      "2022-02-21 17:45:38,514 : INFO : topic #16 (0.044): 0.040*\"joven\" + 0.040*\"seriar\" + 0.040*\"sullivan\" + 0.040*\"mentor\" + 0.040*\"mark\" + 0.040*\"drake\" + 0.040*\"crear\" + 0.040*\"descubrir\" + 0.040*\"adaptacion\" + 0.040*\"victor\"\n",
      "2022-02-21 17:45:38,514 : INFO : topic #5 (0.047): 0.049*\"cintar\" + 0.048*\"manipular\" + 0.028*\"mas\" + 0.026*\"historia\" + 0.025*\"amante\" + 0.025*\"hombre\" + 0.025*\"incluso\" + 0.025*\"antojar\" + 0.025*\"desconocer\" + 0.025*\"asi\"\n",
      "2022-02-21 17:45:38,514 : INFO : topic #7 (0.056): 0.030*\"alterar\" + 0.030*\"mundo\" + 0.020*\"ayudar\" + 0.020*\"mysterio\" + 0.020*\"man\" + 0.020*\"nefasto\" + 0.020*\"pedir\" + 0.020*\"normalidad\" + 0.020*\"volver\" + 0.020*\"despues\"\n",
      "2022-02-21 17:45:38,515 : INFO : topic diff=0.137057, rho=0.447214\n",
      "2022-02-21 17:45:38,519 : INFO : -5.684 per-word bound, 51.4 perplexity estimate based on a held-out corpus of 15 documents with 384 words\n",
      "2022-02-21 17:45:38,519 : INFO : PROGRESS: pass 4, at document #15/15\n",
      "2022-02-21 17:45:38,522 : INFO : optimized alpha [0.036076967, 0.029341085, 0.029341085, 0.032913364, 0.035991676, 0.046315227, 0.03115486, 0.05563552, 0.03124733, 0.031137493, 0.030317726, 0.029341085, 0.041259553, 0.040952604, 0.03825216, 0.03602685, 0.043572284, 0.029341085, 0.029341085, 0.0312665]\n",
      "2022-02-21 17:45:38,523 : INFO : topic #1 (0.029): 0.004*\"orbitar\" + 0.004*\"oportunidad\" + 0.004*\"salvar\" + 0.004*\"salir\" + 0.004*\"provocar\" + 0.004*\"planeta\" + 0.004*\"tierra\" + 0.004*\"improbable\" + 0.004*\"junto\" + 0.004*\"unica\"\n",
      "2022-02-21 17:45:38,523 : INFO : topic #17 (0.029): 0.004*\"orbitar\" + 0.004*\"oportunidad\" + 0.004*\"salvar\" + 0.004*\"salir\" + 0.004*\"provocar\" + 0.004*\"planeta\" + 0.004*\"tierra\" + 0.004*\"improbable\" + 0.004*\"junto\" + 0.004*\"unica\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-21 17:45:38,523 : INFO : topic #16 (0.044): 0.040*\"joven\" + 0.040*\"seriar\" + 0.040*\"sullivan\" + 0.040*\"mentor\" + 0.040*\"mark\" + 0.040*\"drake\" + 0.040*\"crear\" + 0.040*\"descubrir\" + 0.040*\"adaptacion\" + 0.040*\"victor\"\n",
      "2022-02-21 17:45:38,524 : INFO : topic #5 (0.046): 0.050*\"cintar\" + 0.049*\"manipular\" + 0.027*\"mas\" + 0.026*\"historia\" + 0.025*\"amante\" + 0.025*\"hombre\" + 0.025*\"incluso\" + 0.025*\"antojar\" + 0.025*\"desconocer\" + 0.025*\"asi\"\n",
      "2022-02-21 17:45:38,524 : INFO : topic #7 (0.056): 0.030*\"alterar\" + 0.030*\"mundo\" + 0.020*\"ayudar\" + 0.020*\"mysterio\" + 0.020*\"man\" + 0.020*\"nefasto\" + 0.020*\"pedir\" + 0.020*\"normalidad\" + 0.020*\"volver\" + 0.020*\"despues\"\n",
      "2022-02-21 17:45:38,524 : INFO : topic diff=0.109707, rho=0.408248\n",
      "2022-02-21 17:45:38,528 : INFO : -5.625 per-word bound, 49.4 perplexity estimate based on a held-out corpus of 15 documents with 384 words\n",
      "2022-02-21 17:45:38,528 : INFO : PROGRESS: pass 5, at document #15/15\n",
      "2022-02-21 17:45:38,530 : INFO : optimized alpha [0.034852516, 0.027845565, 0.027845565, 0.03103216, 0.034764852, 0.045355216, 0.029469013, 0.055364907, 0.029551474, 0.029453522, 0.028721133, 0.027845565, 0.04069749, 0.040364087, 0.036819063, 0.034801003, 0.042800836, 0.027845565, 0.027845565, 0.029568566]\n",
      "2022-02-21 17:45:38,531 : INFO : topic #17 (0.028): 0.004*\"orbitar\" + 0.004*\"oportunidad\" + 0.004*\"salvar\" + 0.004*\"salir\" + 0.004*\"provocar\" + 0.004*\"planeta\" + 0.004*\"tierra\" + 0.004*\"improbable\" + 0.004*\"junto\" + 0.004*\"unica\"\n",
      "2022-02-21 17:45:38,531 : INFO : topic #18 (0.028): 0.004*\"orbitar\" + 0.004*\"oportunidad\" + 0.004*\"salvar\" + 0.004*\"salir\" + 0.004*\"provocar\" + 0.004*\"planeta\" + 0.004*\"tierra\" + 0.004*\"improbable\" + 0.004*\"junto\" + 0.004*\"unica\"\n",
      "2022-02-21 17:45:38,531 : INFO : topic #16 (0.043): 0.040*\"joven\" + 0.040*\"seriar\" + 0.040*\"sullivan\" + 0.040*\"mentor\" + 0.040*\"mark\" + 0.040*\"drake\" + 0.040*\"crear\" + 0.040*\"descubrir\" + 0.040*\"adaptacion\" + 0.040*\"victor\"\n",
      "2022-02-21 17:45:38,532 : INFO : topic #5 (0.045): 0.050*\"cintar\" + 0.050*\"manipular\" + 0.026*\"mas\" + 0.026*\"historia\" + 0.025*\"amante\" + 0.025*\"hombre\" + 0.025*\"incluso\" + 0.025*\"antojar\" + 0.025*\"desconocer\" + 0.025*\"asi\"\n",
      "2022-02-21 17:45:38,532 : INFO : topic #7 (0.055): 0.030*\"alterar\" + 0.030*\"mundo\" + 0.020*\"ayudar\" + 0.020*\"mysterio\" + 0.020*\"man\" + 0.020*\"nefasto\" + 0.020*\"pedir\" + 0.020*\"normalidad\" + 0.020*\"volver\" + 0.020*\"despues\"\n",
      "2022-02-21 17:45:38,533 : INFO : topic diff=0.085796, rho=0.377964\n",
      "2022-02-21 17:45:38,536 : INFO : -5.593 per-word bound, 48.3 perplexity estimate based on a held-out corpus of 15 documents with 384 words\n",
      "2022-02-21 17:45:38,536 : INFO : PROGRESS: pass 6, at document #15/15\n",
      "2022-02-21 17:45:38,539 : INFO : optimized alpha [0.033781324, 0.026566284, 0.026566284, 0.029443787, 0.03369211, 0.044487525, 0.028036417, 0.05505778, 0.02811086, 0.028022433, 0.027360238, 0.026566284, 0.04017118, 0.039816387, 0.035575934, 0.033728894, 0.042095, 0.026566284, 0.026566284, 0.02812629]\n",
      "2022-02-21 17:45:38,540 : INFO : topic #17 (0.027): 0.004*\"orbitar\" + 0.004*\"oportunidad\" + 0.004*\"salvar\" + 0.004*\"salir\" + 0.004*\"provocar\" + 0.004*\"planeta\" + 0.004*\"tierra\" + 0.004*\"improbable\" + 0.004*\"junto\" + 0.004*\"unica\"\n",
      "2022-02-21 17:45:38,540 : INFO : topic #1 (0.027): 0.004*\"orbitar\" + 0.004*\"oportunidad\" + 0.004*\"salvar\" + 0.004*\"salir\" + 0.004*\"provocar\" + 0.004*\"planeta\" + 0.004*\"tierra\" + 0.004*\"improbable\" + 0.004*\"junto\" + 0.004*\"unica\"\n",
      "2022-02-21 17:45:38,541 : INFO : topic #16 (0.042): 0.040*\"joven\" + 0.040*\"seriar\" + 0.040*\"sullivan\" + 0.040*\"mentor\" + 0.040*\"mark\" + 0.040*\"drake\" + 0.040*\"crear\" + 0.040*\"descubrir\" + 0.040*\"adaptacion\" + 0.040*\"victor\"\n",
      "2022-02-21 17:45:38,541 : INFO : topic #5 (0.044): 0.050*\"cintar\" + 0.050*\"manipular\" + 0.026*\"mas\" + 0.026*\"historia\" + 0.026*\"amante\" + 0.026*\"hombre\" + 0.026*\"incluso\" + 0.026*\"antojar\" + 0.026*\"desconocer\" + 0.026*\"asi\"\n",
      "2022-02-21 17:45:38,541 : INFO : topic #7 (0.055): 0.030*\"alterar\" + 0.030*\"mundo\" + 0.020*\"ayudar\" + 0.020*\"mysterio\" + 0.020*\"man\" + 0.020*\"nefasto\" + 0.020*\"pedir\" + 0.020*\"normalidad\" + 0.020*\"volver\" + 0.020*\"despues\"\n",
      "2022-02-21 17:45:38,542 : INFO : topic diff=0.065074, rho=0.353553\n",
      "2022-02-21 17:45:38,545 : INFO : -5.574 per-word bound, 47.6 perplexity estimate based on a held-out corpus of 15 documents with 384 words\n",
      "2022-02-21 17:45:38,545 : INFO : PROGRESS: pass 7, at document #15/15\n",
      "2022-02-21 17:45:38,547 : INFO : optimized alpha [0.03283101, 0.025453173, 0.025453173, 0.028076833, 0.03274082, 0.0436958, 0.026796859, 0.054726932, 0.02686472, 0.026784107, 0.026179682, 0.025453173, 0.03967619, 0.039303925, 0.034481104, 0.032778006, 0.041444127, 0.025453173, 0.025453173, 0.026878782]\n",
      "2022-02-21 17:45:38,548 : INFO : topic #18 (0.025): 0.004*\"orbitar\" + 0.004*\"oportunidad\" + 0.004*\"salvar\" + 0.004*\"salir\" + 0.004*\"provocar\" + 0.004*\"planeta\" + 0.004*\"tierra\" + 0.004*\"improbable\" + 0.004*\"junto\" + 0.004*\"unica\"\n",
      "2022-02-21 17:45:38,548 : INFO : topic #17 (0.025): 0.004*\"orbitar\" + 0.004*\"oportunidad\" + 0.004*\"salvar\" + 0.004*\"salir\" + 0.004*\"provocar\" + 0.004*\"planeta\" + 0.004*\"tierra\" + 0.004*\"improbable\" + 0.004*\"junto\" + 0.004*\"unica\"\n",
      "2022-02-21 17:45:38,549 : INFO : topic #16 (0.041): 0.040*\"joven\" + 0.040*\"seriar\" + 0.040*\"sullivan\" + 0.040*\"mentor\" + 0.040*\"mark\" + 0.040*\"drake\" + 0.040*\"crear\" + 0.040*\"descubrir\" + 0.040*\"adaptacion\" + 0.040*\"victor\"\n",
      "2022-02-21 17:45:38,549 : INFO : topic #5 (0.044): 0.051*\"cintar\" + 0.051*\"manipular\" + 0.026*\"mas\" + 0.026*\"historia\" + 0.026*\"amante\" + 0.026*\"hombre\" + 0.026*\"incluso\" + 0.026*\"antojar\" + 0.026*\"desconocer\" + 0.026*\"asi\"\n",
      "2022-02-21 17:45:38,549 : INFO : topic #7 (0.055): 0.030*\"alterar\" + 0.030*\"mundo\" + 0.020*\"ayudar\" + 0.020*\"mysterio\" + 0.020*\"man\" + 0.020*\"nefasto\" + 0.020*\"pedir\" + 0.020*\"normalidad\" + 0.020*\"volver\" + 0.020*\"despues\"\n",
      "2022-02-21 17:45:38,550 : INFO : topic diff=0.048098, rho=0.333333\n",
      "2022-02-21 17:45:38,553 : INFO : -5.564 per-word bound, 47.3 perplexity estimate based on a held-out corpus of 15 documents with 384 words\n",
      "2022-02-21 17:45:38,554 : INFO : PROGRESS: pass 8, at document #15/15\n",
      "2022-02-21 17:45:38,556 : INFO : optimized alpha [0.031978447, 0.024471456, 0.024471456, 0.026882559, 0.031887695, 0.042967793, 0.025708875, 0.054381043, 0.025771223, 0.025697157, 0.025141185, 0.024471456, 0.039208893, 0.038822256, 0.03350515, 0.03192511, 0.040840067, 0.024471456, 0.024471456, 0.025784142]\n",
      "2022-02-21 17:45:38,557 : INFO : topic #2 (0.024): 0.004*\"orbitar\" + 0.004*\"oportunidad\" + 0.004*\"salvar\" + 0.004*\"salir\" + 0.004*\"provocar\" + 0.004*\"planeta\" + 0.004*\"tierra\" + 0.004*\"improbable\" + 0.004*\"junto\" + 0.004*\"unica\"\n",
      "2022-02-21 17:45:38,557 : INFO : topic #17 (0.024): 0.004*\"orbitar\" + 0.004*\"oportunidad\" + 0.004*\"salvar\" + 0.004*\"salir\" + 0.004*\"provocar\" + 0.004*\"planeta\" + 0.004*\"tierra\" + 0.004*\"improbable\" + 0.004*\"junto\" + 0.004*\"unica\"\n",
      "2022-02-21 17:45:38,558 : INFO : topic #16 (0.041): 0.040*\"joven\" + 0.040*\"seriar\" + 0.040*\"sullivan\" + 0.040*\"mentor\" + 0.040*\"mark\" + 0.040*\"drake\" + 0.040*\"crear\" + 0.040*\"descubrir\" + 0.040*\"adaptacion\" + 0.040*\"victor\"\n",
      "2022-02-21 17:45:38,558 : INFO : topic #5 (0.043): 0.051*\"cintar\" + 0.051*\"manipular\" + 0.026*\"mas\" + 0.026*\"historia\" + 0.026*\"amante\" + 0.026*\"hombre\" + 0.026*\"incluso\" + 0.026*\"antojar\" + 0.026*\"desconocer\" + 0.026*\"asi\"\n",
      "2022-02-21 17:45:38,558 : INFO : topic #7 (0.054): 0.030*\"alterar\" + 0.030*\"mundo\" + 0.020*\"ayudar\" + 0.020*\"mysterio\" + 0.020*\"man\" + 0.020*\"nefasto\" + 0.020*\"pedir\" + 0.020*\"normalidad\" + 0.020*\"volver\" + 0.020*\"despues\"\n",
      "2022-02-21 17:45:38,559 : INFO : topic diff=0.034937, rho=0.316228\n",
      "2022-02-21 17:45:38,562 : INFO : -5.557 per-word bound, 47.1 perplexity estimate based on a held-out corpus of 15 documents with 384 words\n",
      "2022-02-21 17:45:38,563 : INFO : PROGRESS: pass 9, at document #15/15\n",
      "2022-02-21 17:45:38,565 : INFO : optimized alpha [0.031206528, 0.02359602, 0.02359602, 0.02582632, 0.031115515, 0.042294074, 0.024742758, 0.054026153, 0.02480042, 0.024731921, 0.024217224, 0.02359602, 0.03876629, 0.03836777, 0.0326265, 0.031153036, 0.04027644, 0.02359602, 0.02359602, 0.024812367]\n",
      "2022-02-21 17:45:38,565 : INFO : topic #11 (0.024): 0.004*\"orbitar\" + 0.004*\"oportunidad\" + 0.004*\"salvar\" + 0.004*\"salir\" + 0.004*\"provocar\" + 0.004*\"planeta\" + 0.004*\"tierra\" + 0.004*\"improbable\" + 0.004*\"junto\" + 0.004*\"unica\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-21 17:45:38,565 : INFO : topic #17 (0.024): 0.004*\"orbitar\" + 0.004*\"oportunidad\" + 0.004*\"salvar\" + 0.004*\"salir\" + 0.004*\"provocar\" + 0.004*\"planeta\" + 0.004*\"tierra\" + 0.004*\"improbable\" + 0.004*\"junto\" + 0.004*\"unica\"\n",
      "2022-02-21 17:45:38,566 : INFO : topic #16 (0.040): 0.040*\"joven\" + 0.040*\"seriar\" + 0.040*\"sullivan\" + 0.040*\"mentor\" + 0.040*\"mark\" + 0.040*\"drake\" + 0.040*\"crear\" + 0.040*\"descubrir\" + 0.040*\"adaptacion\" + 0.040*\"victor\"\n",
      "2022-02-21 17:45:38,566 : INFO : topic #5 (0.042): 0.051*\"cintar\" + 0.051*\"manipular\" + 0.026*\"mas\" + 0.026*\"historia\" + 0.026*\"amante\" + 0.026*\"hombre\" + 0.026*\"incluso\" + 0.026*\"antojar\" + 0.026*\"desconocer\" + 0.026*\"asi\"\n",
      "2022-02-21 17:45:38,566 : INFO : topic #7 (0.054): 0.030*\"alterar\" + 0.030*\"mundo\" + 0.020*\"ayudar\" + 0.020*\"mysterio\" + 0.020*\"man\" + 0.020*\"nefasto\" + 0.020*\"pedir\" + 0.020*\"normalidad\" + 0.020*\"volver\" + 0.020*\"despues\"\n",
      "2022-02-21 17:45:38,566 : INFO : topic diff=0.025139, rho=0.301511\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Enable logging so we can see the progress of ducment convergence and it can help us tune our parameters for model\"\"\"\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "\n",
    "#If you set passes = 10 you will see this line 10 times. \n",
    "#Make sure that by the final passes, most of the documents have converged. \n",
    "#So you want to choose both passes and iterations to be high enough for this to happen.\"\"\"\n",
    "\n",
    "\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=20, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                            eval_every=1,                                        \n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "66pL2fHX_0zi",
    "outputId": "1bf1e604-329a-4d12-9fb2-db25254cea9a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-21 17:45:38,570 : INFO : topic #0 (0.031): 0.049*\"heroes\" + 0.025*\"mision\" + 0.025*\"partir\" + 0.025*\"criminal\" + 0.025*\"oseon\" + 0.025*\"humanize\" + 0.025*\"seleccion\" + 0.025*\"pretender\" + 0.025*\"poseedor\" + 0.025*\"tambien\"\n",
      "2022-02-21 17:45:38,570 : INFO : topic #1 (0.024): 0.004*\"orbitar\" + 0.004*\"oportunidad\" + 0.004*\"salvar\" + 0.004*\"salir\" + 0.004*\"provocar\" + 0.004*\"planeta\" + 0.004*\"tierra\" + 0.004*\"improbable\" + 0.004*\"junto\" + 0.004*\"unica\"\n",
      "2022-02-21 17:45:38,571 : INFO : topic #2 (0.024): 0.004*\"orbitar\" + 0.004*\"oportunidad\" + 0.004*\"salvar\" + 0.004*\"salir\" + 0.004*\"provocar\" + 0.004*\"planeta\" + 0.004*\"tierra\" + 0.004*\"improbable\" + 0.004*\"junto\" + 0.004*\"unica\"\n",
      "2022-02-21 17:45:38,571 : INFO : topic #3 (0.026): 0.005*\"encantar\" + 0.005*\"miembro\" + 0.005*\"increible\" + 0.005*\"aventurar\" + 0.005*\"situa\" + 0.005*\"magicos\" + 0.005*\"poder\" + 0.005*\"narrar\" + 0.005*\"protagonista\" + 0.005*\"corazon\"\n",
      "2022-02-21 17:45:38,572 : INFO : topic #4 (0.031): 0.031*\"deberan\" + 0.031*\"sera\" + 0.031*\"junto\" + 0.031*\"colosal\" + 0.031*\"oportunidad\" + 0.031*\"trabajar\" + 0.031*\"provocar\" + 0.031*\"planeta\" + 0.031*\"orbitar\" + 0.031*\"contar\"\n",
      "2022-02-21 17:45:38,572 : INFO : topic #5 (0.042): 0.051*\"cintar\" + 0.051*\"manipular\" + 0.026*\"mas\" + 0.026*\"historia\" + 0.026*\"amante\" + 0.026*\"hombre\" + 0.026*\"incluso\" + 0.026*\"antojar\" + 0.026*\"desconocer\" + 0.026*\"asi\"\n",
      "2022-02-21 17:45:38,572 : INFO : topic #6 (0.025): 0.004*\"identidad\" + 0.004*\"recuperar\" + 0.004*\"accion\" + 0.004*\"tendra\" + 0.004*\"casar\" + 0.004*\"lejos\" + 0.004*\"desesperar\" + 0.004*\"doctor\" + 0.004*\"peter\" + 0.004*\"heroe\"\n",
      "2022-02-21 17:45:38,572 : INFO : topic #7 (0.054): 0.030*\"alterar\" + 0.030*\"mundo\" + 0.020*\"ayudar\" + 0.020*\"mysterio\" + 0.020*\"man\" + 0.020*\"nefasto\" + 0.020*\"pedir\" + 0.020*\"normalidad\" + 0.020*\"volver\" + 0.020*\"despues\"\n",
      "2022-02-21 17:45:38,573 : INFO : topic #8 (0.025): 0.005*\"vada\" + 0.005*\"cerrar\" + 0.005*\"secundario\" + 0.005*\"edad\" + 0.005*\"enfrentarse\" + 0.005*\"siempre\" + 0.005*\"problema\" + 0.005*\"tragedia\" + 0.005*\"vision\" + 0.005*\"propio\"\n",
      "2022-02-21 17:45:38,573 : INFO : topic #9 (0.025): 0.004*\"casarse\" + 0.004*\"kat\" + 0.004*\"bastian\" + 0.004*\"seguidor\" + 0.004*\"absolutamente\" + 0.004*\"gran\" + 0.004*\"repentinamente\" + 0.004*\"dejar\" + 0.004*\"plantar\" + 0.004*\"conocer\"\n",
      "2022-02-21 17:45:38,573 : INFO : topic #10 (0.024): 0.004*\"manipular\" + 0.004*\"cintar\" + 0.004*\"amante\" + 0.004*\"manipulador\" + 0.004*\"sera\" + 0.004*\"antojar\" + 0.004*\"contaria\" + 0.004*\"argumentar\" + 0.004*\"convertir\" + 0.004*\"disfrutar\"\n",
      "2022-02-21 17:45:38,574 : INFO : topic #11 (0.024): 0.004*\"orbitar\" + 0.004*\"oportunidad\" + 0.004*\"salvar\" + 0.004*\"salir\" + 0.004*\"provocar\" + 0.004*\"planeta\" + 0.004*\"tierra\" + 0.004*\"improbable\" + 0.004*\"junto\" + 0.004*\"unica\"\n",
      "2022-02-21 17:45:38,574 : INFO : topic #12 (0.039): 0.066*\"mas\" + 0.066*\"nunca\" + 0.033*\"seguir\" + 0.033*\"seriar\" + 0.033*\"acontecimiento\" + 0.033*\"radicalmente\" + 0.033*\"cambiar\" + 0.033*\"vida\" + 0.033*\"surgir\" + 0.033*\"casi\"\n",
      "2022-02-21 17:45:38,574 : INFO : topic #13 (0.038): 0.052*\"increible\" + 0.052*\"miembro\" + 0.052*\"familia\" + 0.052*\"magicos\" + 0.052*\"aventurar\" + 0.052*\"brasil\" + 0.052*\"clan\" + 0.052*\"joven\" + 0.052*\"situa\" + 0.052*\"narrar\"\n",
      "2022-02-21 17:45:38,575 : INFO : topic #14 (0.033): 0.048*\"seguidor\" + 0.048*\"bastian\" + 0.048*\"kat\" + 0.048*\"casarse\" + 0.025*\"lopez\" + 0.025*\"garden\" + 0.025*\"artista\" + 0.025*\"dia\" + 0.025*\"decidir\" + 0.025*\"llegar\"\n",
      "2022-02-21 17:45:38,575 : INFO : topic #15 (0.031): 0.056*\"mitad\" + 0.056*\"humanar\" + 0.029*\"juntar\" + 0.029*\"hotel\" + 0.029*\"cuartar\" + 0.029*\"siempre\" + 0.029*\"ademar\" + 0.029*\"franquicia\" + 0.029*\"frankenstein\" + 0.029*\"animacion\"\n",
      "2022-02-21 17:45:38,575 : INFO : topic #16 (0.040): 0.040*\"joven\" + 0.040*\"seriar\" + 0.040*\"sullivan\" + 0.040*\"mentor\" + 0.040*\"mark\" + 0.040*\"drake\" + 0.040*\"crear\" + 0.040*\"descubrir\" + 0.040*\"adaptacion\" + 0.040*\"victor\"\n",
      "2022-02-21 17:45:38,575 : INFO : topic #17 (0.024): 0.004*\"orbitar\" + 0.004*\"oportunidad\" + 0.004*\"salvar\" + 0.004*\"salir\" + 0.004*\"provocar\" + 0.004*\"planeta\" + 0.004*\"tierra\" + 0.004*\"improbable\" + 0.004*\"junto\" + 0.004*\"unica\"\n",
      "2022-02-21 17:45:38,576 : INFO : topic #18 (0.024): 0.004*\"orbitar\" + 0.004*\"oportunidad\" + 0.004*\"salvar\" + 0.004*\"salir\" + 0.004*\"provocar\" + 0.004*\"planeta\" + 0.004*\"tierra\" + 0.004*\"improbable\" + 0.004*\"junto\" + 0.004*\"unica\"\n",
      "2022-02-21 17:45:38,576 : INFO : topic #19 (0.025): 0.005*\"naughty\" + 0.005*\"dog\" + 0.005*\"amigar\" + 0.005*\"nathan\" + 0.005*\"videojuego\" + 0.005*\"saga\" + 0.005*\"wahlberg\" + 0.005*\"detalle\" + 0.005*\"conocer\" + 0.005*\"exitoso\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.049*\"heroes\" + 0.025*\"mision\" + 0.025*\"partir\" + 0.025*\"criminal\" + 0.025*\"oseon\" + 0.025*\"humanize\" + 0.025*\"seleccion\" + 0.025*\"pretender\" + 0.025*\"poseedor\" + 0.025*\"tambien\"'), (1, '0.004*\"orbitar\" + 0.004*\"oportunidad\" + 0.004*\"salvar\" + 0.004*\"salir\" + 0.004*\"provocar\" + 0.004*\"planeta\" + 0.004*\"tierra\" + 0.004*\"improbable\" + 0.004*\"junto\" + 0.004*\"unica\"'), (2, '0.004*\"orbitar\" + 0.004*\"oportunidad\" + 0.004*\"salvar\" + 0.004*\"salir\" + 0.004*\"provocar\" + 0.004*\"planeta\" + 0.004*\"tierra\" + 0.004*\"improbable\" + 0.004*\"junto\" + 0.004*\"unica\"'), (3, '0.005*\"encantar\" + 0.005*\"miembro\" + 0.005*\"increible\" + 0.005*\"aventurar\" + 0.005*\"situa\" + 0.005*\"magicos\" + 0.005*\"poder\" + 0.005*\"narrar\" + 0.005*\"protagonista\" + 0.005*\"corazon\"'), (4, '0.031*\"deberan\" + 0.031*\"sera\" + 0.031*\"junto\" + 0.031*\"colosal\" + 0.031*\"oportunidad\" + 0.031*\"trabajar\" + 0.031*\"provocar\" + 0.031*\"planeta\" + 0.031*\"orbitar\" + 0.031*\"contar\"'), (5, '0.051*\"cintar\" + 0.051*\"manipular\" + 0.026*\"mas\" + 0.026*\"historia\" + 0.026*\"amante\" + 0.026*\"hombre\" + 0.026*\"incluso\" + 0.026*\"antojar\" + 0.026*\"desconocer\" + 0.026*\"asi\"'), (6, '0.004*\"identidad\" + 0.004*\"recuperar\" + 0.004*\"accion\" + 0.004*\"tendra\" + 0.004*\"casar\" + 0.004*\"lejos\" + 0.004*\"desesperar\" + 0.004*\"doctor\" + 0.004*\"peter\" + 0.004*\"heroe\"'), (7, '0.030*\"alterar\" + 0.030*\"mundo\" + 0.020*\"ayudar\" + 0.020*\"mysterio\" + 0.020*\"man\" + 0.020*\"nefasto\" + 0.020*\"pedir\" + 0.020*\"normalidad\" + 0.020*\"volver\" + 0.020*\"despues\"'), (8, '0.005*\"vada\" + 0.005*\"cerrar\" + 0.005*\"secundario\" + 0.005*\"edad\" + 0.005*\"enfrentarse\" + 0.005*\"siempre\" + 0.005*\"problema\" + 0.005*\"tragedia\" + 0.005*\"vision\" + 0.005*\"propio\"'), (9, '0.004*\"casarse\" + 0.004*\"kat\" + 0.004*\"bastian\" + 0.004*\"seguidor\" + 0.004*\"absolutamente\" + 0.004*\"gran\" + 0.004*\"repentinamente\" + 0.004*\"dejar\" + 0.004*\"plantar\" + 0.004*\"conocer\"'), (10, '0.004*\"manipular\" + 0.004*\"cintar\" + 0.004*\"amante\" + 0.004*\"manipulador\" + 0.004*\"sera\" + 0.004*\"antojar\" + 0.004*\"contaria\" + 0.004*\"argumentar\" + 0.004*\"convertir\" + 0.004*\"disfrutar\"'), (11, '0.004*\"orbitar\" + 0.004*\"oportunidad\" + 0.004*\"salvar\" + 0.004*\"salir\" + 0.004*\"provocar\" + 0.004*\"planeta\" + 0.004*\"tierra\" + 0.004*\"improbable\" + 0.004*\"junto\" + 0.004*\"unica\"'), (12, '0.066*\"mas\" + 0.066*\"nunca\" + 0.033*\"seguir\" + 0.033*\"seriar\" + 0.033*\"acontecimiento\" + 0.033*\"radicalmente\" + 0.033*\"cambiar\" + 0.033*\"vida\" + 0.033*\"surgir\" + 0.033*\"casi\"'), (13, '0.052*\"increible\" + 0.052*\"miembro\" + 0.052*\"familia\" + 0.052*\"magicos\" + 0.052*\"aventurar\" + 0.052*\"brasil\" + 0.052*\"clan\" + 0.052*\"joven\" + 0.052*\"situa\" + 0.052*\"narrar\"'), (14, '0.048*\"seguidor\" + 0.048*\"bastian\" + 0.048*\"kat\" + 0.048*\"casarse\" + 0.025*\"lopez\" + 0.025*\"garden\" + 0.025*\"artista\" + 0.025*\"dia\" + 0.025*\"decidir\" + 0.025*\"llegar\"'), (15, '0.056*\"mitad\" + 0.056*\"humanar\" + 0.029*\"juntar\" + 0.029*\"hotel\" + 0.029*\"cuartar\" + 0.029*\"siempre\" + 0.029*\"ademar\" + 0.029*\"franquicia\" + 0.029*\"frankenstein\" + 0.029*\"animacion\"'), (16, '0.040*\"joven\" + 0.040*\"seriar\" + 0.040*\"sullivan\" + 0.040*\"mentor\" + 0.040*\"mark\" + 0.040*\"drake\" + 0.040*\"crear\" + 0.040*\"descubrir\" + 0.040*\"adaptacion\" + 0.040*\"victor\"'), (17, '0.004*\"orbitar\" + 0.004*\"oportunidad\" + 0.004*\"salvar\" + 0.004*\"salir\" + 0.004*\"provocar\" + 0.004*\"planeta\" + 0.004*\"tierra\" + 0.004*\"improbable\" + 0.004*\"junto\" + 0.004*\"unica\"'), (18, '0.004*\"orbitar\" + 0.004*\"oportunidad\" + 0.004*\"salvar\" + 0.004*\"salir\" + 0.004*\"provocar\" + 0.004*\"planeta\" + 0.004*\"tierra\" + 0.004*\"improbable\" + 0.004*\"junto\" + 0.004*\"unica\"'), (19, '0.005*\"naughty\" + 0.005*\"dog\" + 0.005*\"amigar\" + 0.005*\"nathan\" + 0.005*\"videojuego\" + 0.005*\"saga\" + 0.005*\"wahlberg\" + 0.005*\"detalle\" + 0.005*\"conocer\" + 0.005*\"exitoso\"')]\n"
     ]
    }
   ],
   "source": [
    "# Print the Keyword in the 10 topics\n",
    "print(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C_B8KVQL__d3"
   },
   "source": [
    "comment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m8TjpwmEAAe_",
    "outputId": "defa5749-180f-486f-8ffe-f6aaa7387198"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-21 17:45:38,584 : INFO : -5.553 per-word bound, 46.9 perplexity estimate based on a held-out corpus of 15 documents with 384 words\n",
      "2022-02-21 17:45:38,587 : INFO : using ParallelWordOccurrenceAccumulator(processes=7, batch_size=64) to estimate probabilities from sliding windows\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perplexity:  -5.552844027367731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-21 17:45:38,723 : INFO : serializing accumulator to return to master...\n",
      "2022-02-21 17:45:38,725 : INFO : serializing accumulator to return to master...\n",
      "2022-02-21 17:45:38,724 : INFO : serializing accumulator to return to master...\n",
      "2022-02-21 17:45:38,723 : INFO : serializing accumulator to return to master...\n",
      "2022-02-21 17:45:38,724 : INFO : serializing accumulator to return to master...\n",
      "2022-02-21 17:45:38,725 : INFO : serializing accumulator to return to master...\n",
      "2022-02-21 17:45:38,767 : INFO : serializing accumulator to return to master...\n",
      "2022-02-21 17:45:38,758 : INFO : accumulator serialized\n",
      "2022-02-21 17:45:38,759 : INFO : accumulator serialized\n",
      "2022-02-21 17:45:38,765 : INFO : accumulator serialized\n",
      "2022-02-21 17:45:38,822 : INFO : 7 accumulators retrieved from output queue\n",
      "2022-02-21 17:45:38,759 : INFO : accumulator serialized\n",
      "2022-02-21 17:45:38,757 : INFO : accumulator serialized\n",
      "2022-02-21 17:45:38,757 : INFO : accumulator serialized\n",
      "2022-02-21 17:45:38,774 : INFO : accumulator serialized\n",
      "2022-02-21 17:45:38,844 : INFO : accumulated word occurrence stats for 15 virtual documents\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Coherence Score:  0.8043368443079659\n"
     ]
    }
   ],
   "source": [
    "## Compute Model Perplexity and Coherence Score\n",
    "\n",
    "\n",
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting watermark\n",
      "  Downloading watermark-2.3.0-py2.py3-none-any.whl (7.2 kB)\n",
      "Requirement already satisfied: ipython in /Users/polina/opt/anaconda3/lib/python3.7/site-packages (from watermark) (7.12.0)\n",
      "Requirement already satisfied: importlib-metadata<3.0; python_version < \"3.8\" in /Users/polina/opt/anaconda3/lib/python3.7/site-packages (from watermark) (1.5.0)\n",
      "Requirement already satisfied: traitlets>=4.2 in /Users/polina/opt/anaconda3/lib/python3.7/site-packages (from ipython->watermark) (5.1.1)\n",
      "Requirement already satisfied: pygments in /Users/polina/opt/anaconda3/lib/python3.7/site-packages (from ipython->watermark) (2.5.2)\n",
      "Requirement already satisfied: jedi>=0.10 in /Users/polina/opt/anaconda3/lib/python3.7/site-packages (from ipython->watermark) (0.14.1)\n",
      "Requirement already satisfied: backcall in /Users/polina/opt/anaconda3/lib/python3.7/site-packages (from ipython->watermark) (0.1.0)\n",
      "Requirement already satisfied: pickleshare in /Users/polina/opt/anaconda3/lib/python3.7/site-packages (from ipython->watermark) (0.7.5)\n",
      "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /Users/polina/opt/anaconda3/lib/python3.7/site-packages (from ipython->watermark) (4.8.0)\n",
      "Requirement already satisfied: decorator in /Users/polina/opt/anaconda3/lib/python3.7/site-packages (from ipython->watermark) (4.4.1)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /Users/polina/opt/anaconda3/lib/python3.7/site-packages (from ipython->watermark) (3.0.3)\n",
      "Requirement already satisfied: appnope; sys_platform == \"darwin\" in /Users/polina/opt/anaconda3/lib/python3.7/site-packages (from ipython->watermark) (0.1.0)\n",
      "Requirement already satisfied: setuptools>=18.5 in /Users/polina/opt/anaconda3/lib/python3.7/site-packages (from ipython->watermark) (46.0.0.post20200309)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/polina/opt/anaconda3/lib/python3.7/site-packages (from importlib-metadata<3.0; python_version < \"3.8\"->watermark) (2.2.0)\n",
      "Requirement already satisfied: parso>=0.5.0 in /Users/polina/opt/anaconda3/lib/python3.7/site-packages (from jedi>=0.10->ipython->watermark) (0.5.2)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /Users/polina/opt/anaconda3/lib/python3.7/site-packages (from pexpect; sys_platform != \"win32\"->ipython->watermark) (0.6.0)\n",
      "Requirement already satisfied: wcwidth in /Users/polina/opt/anaconda3/lib/python3.7/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->watermark) (0.1.8)\n",
      "Installing collected packages: watermark\n",
      "Successfully installed watermark-2.3.0\n",
      "Python implementation: CPython\n",
      "Python version       : 3.7.6\n",
      "IPython version      : 7.12.0\n",
      "\n",
      "wget         : not installed\n",
      "pandas       : 1.3.5\n",
      "numpy        : 1.20.0\n",
      "geopy        : not installed\n",
      "altair       : 4.1.0\n",
      "vega         : not installed\n",
      "vega_datasets: not installed\n",
      "watermark    : 2.3.0\n",
      "\n",
      "Compiler    : Clang 4.0.1 (tags/RELEASE_401/final)\n",
      "OS          : Darwin\n",
      "Release     : 20.2.0\n",
      "Machine     : x86_64\n",
      "Processor   : i386\n",
      "CPU cores   : 8\n",
      "Architecture: 64bit\n",
      "\n",
      " \n",
      "Last updated: Mon Feb 21 2022 17:49:05CET\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install watermark\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The watermark extension is already loaded. To reload it, use:\n",
      "  %reload_ext watermark\n",
      "Python implementation: CPython\n",
      "Python version       : 3.7.6\n",
      "IPython version      : 7.12.0\n",
      "\n",
      "wget         : not installed\n",
      "pandas       : 1.3.5\n",
      "numpy        : 1.20.0\n",
      "geopy        : not installed\n",
      "altair       : 4.1.0\n",
      "vega         : not installed\n",
      "vega_datasets: not installed\n",
      "watermark    : 2.3.0\n",
      "\n",
      "Compiler    : Clang 4.0.1 (tags/RELEASE_401/final)\n",
      "OS          : Darwin\n",
      "Release     : 20.2.0\n",
      "Machine     : x86_64\n",
      "Processor   : i386\n",
      "CPU cores   : 8\n",
      "Architecture: 64bit\n",
      "\n",
      " \n",
      "Last updated: Mon Feb 21 2022 17:49:31CET\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "\n",
    "# python, ipython, packages, and machine characteristics\n",
    "%watermark -v -m -p wget,pandas,numpy,geopy,altair,vega,vega_datasets,watermark \n",
    "\n",
    "# date\n",
    "print (\" \")\n",
    "%watermark -u -n -t -z "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7uUVT315AFrW"
   },
   "source": [
    "Visualize the topics-**keywords**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RCBHgFD9AL8q",
    "outputId": "12d3c4d9-8951-469f-fdc0-9d66c1549f96"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyLDAvis in /Users/polina/opt/anaconda3/lib/python3.7/site-packages (3.3.1)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /Users/polina/opt/anaconda3/lib/python3.7/site-packages (from pyLDAvis) (1.20.0)\n",
      "Requirement already satisfied: sklearn in /Users/polina/opt/anaconda3/lib/python3.7/site-packages (from pyLDAvis) (0.0)\n",
      "Requirement already satisfied: pandas>=1.2.0 in /Users/polina/opt/anaconda3/lib/python3.7/site-packages (from pyLDAvis) (1.2.4)\n",
      "Requirement already satisfied: numexpr in /Users/polina/opt/anaconda3/lib/python3.7/site-packages (from pyLDAvis) (2.7.1)\n",
      "Requirement already satisfied: gensim in /Users/polina/opt/anaconda3/lib/python3.7/site-packages (from pyLDAvis) (3.8.0)\n",
      "Requirement already satisfied: setuptools in /Users/polina/opt/anaconda3/lib/python3.7/site-packages (from pyLDAvis) (46.0.0.post20200309)\n",
      "Requirement already satisfied: jinja2 in /Users/polina/opt/anaconda3/lib/python3.7/site-packages (from pyLDAvis) (2.11.1)\n",
      "Requirement already satisfied: joblib in /Users/polina/opt/anaconda3/lib/python3.7/site-packages (from pyLDAvis) (0.14.1)\n",
      "Requirement already satisfied: scikit-learn in /Users/polina/opt/anaconda3/lib/python3.7/site-packages (from pyLDAvis) (0.22.1)\n",
      "Requirement already satisfied: scipy in /Users/polina/opt/anaconda3/lib/python3.7/site-packages (from pyLDAvis) (1.4.1)\n",
      "Requirement already satisfied: future in /Users/polina/opt/anaconda3/lib/python3.7/site-packages (from pyLDAvis) (0.18.2)\n",
      "Requirement already satisfied: funcy in /Users/polina/opt/anaconda3/lib/python3.7/site-packages (from pyLDAvis) (1.16)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /Users/polina/opt/anaconda3/lib/python3.7/site-packages (from pandas>=1.2.0->pyLDAvis) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in /Users/polina/opt/anaconda3/lib/python3.7/site-packages (from pandas>=1.2.0->pyLDAvis) (2019.3)\n",
      "Requirement already satisfied: six>=1.5.0 in /Users/polina/opt/anaconda3/lib/python3.7/site-packages (from gensim->pyLDAvis) (1.15.0)\n",
      "Requirement already satisfied: smart-open>=1.7.0 in /Users/polina/opt/anaconda3/lib/python3.7/site-packages (from gensim->pyLDAvis) (1.9.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /Users/polina/opt/anaconda3/lib/python3.7/site-packages (from jinja2->pyLDAvis) (1.1.1)\n",
      "Requirement already satisfied: requests in /Users/polina/opt/anaconda3/lib/python3.7/site-packages (from smart-open>=1.7.0->gensim->pyLDAvis) (2.22.0)\n",
      "Requirement already satisfied: boto>=2.32 in /Users/polina/opt/anaconda3/lib/python3.7/site-packages (from smart-open>=1.7.0->gensim->pyLDAvis) (2.49.0)\n",
      "Requirement already satisfied: boto3 in /Users/polina/opt/anaconda3/lib/python3.7/site-packages (from smart-open>=1.7.0->gensim->pyLDAvis) (1.17.56)\n",
      "Requirement already satisfied: bz2file in /Users/polina/opt/anaconda3/lib/python3.7/site-packages (from smart-open>=1.7.0->gensim->pyLDAvis) (0.98)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/polina/opt/anaconda3/lib/python3.7/site-packages (from requests->smart-open>=1.7.0->gensim->pyLDAvis) (2019.11.28)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /Users/polina/opt/anaconda3/lib/python3.7/site-packages (from requests->smart-open>=1.7.0->gensim->pyLDAvis) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /Users/polina/opt/anaconda3/lib/python3.7/site-packages (from requests->smart-open>=1.7.0->gensim->pyLDAvis) (2.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/polina/opt/anaconda3/lib/python3.7/site-packages (from requests->smart-open>=1.7.0->gensim->pyLDAvis) (1.25.8)\n",
      "Requirement already satisfied: s3transfer<0.5.0,>=0.4.0 in /Users/polina/opt/anaconda3/lib/python3.7/site-packages (from boto3->smart-open>=1.7.0->gensim->pyLDAvis) (0.4.2)\n",
      "Requirement already satisfied: botocore<1.21.0,>=1.20.56 in /Users/polina/opt/anaconda3/lib/python3.7/site-packages (from boto3->smart-open>=1.7.0->gensim->pyLDAvis) (1.20.56)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /Users/polina/opt/anaconda3/lib/python3.7/site-packages (from boto3->smart-open>=1.7.0->gensim->pyLDAvis) (0.10.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 378
    },
    "id": "skrlAuJBJDiB",
    "outputId": "8fd30aba-4960-48f3-f76f-55cb553f9340"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas==1.3.1\n",
      "  Downloading pandas-1.3.1-cp37-cp37m-macosx_10_9_x86_64.whl (11.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 11.0 MB 1.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pytz>=2017.3 in /Users/polina/opt/anaconda3/lib/python3.7/site-packages (from pandas==1.3.1) (2019.3)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /Users/polina/opt/anaconda3/lib/python3.7/site-packages (from pandas==1.3.1) (1.20.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /Users/polina/opt/anaconda3/lib/python3.7/site-packages (from pandas==1.3.1) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/polina/opt/anaconda3/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas==1.3.1) (1.15.0)\n",
      "\u001b[31mERROR: sklearn-pandas 2.2.0 has requirement scikit-learn>=0.23.0, but you'll have scikit-learn 0.22.1 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: sklearn-pandas 2.2.0 has requirement scipy>=1.5.1, but you'll have scipy 1.4.1 which is incompatible.\u001b[0m\n",
      "Installing collected packages: pandas\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 1.2.4\n",
      "    Uninstalling pandas-1.2.4:\n",
      "      Successfully uninstalled pandas-1.2.4\n",
      "Successfully installed pandas-1.3.1\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas==1.3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 463
    },
    "id": "malpChBVADGT",
    "outputId": "8cfc76b2-ce64-4f50-ecc9-87c41cc488ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-1.3.5-cp37-cp37m-macosx_10_9_x86_64.whl (11.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 11.0 MB 281 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: python-dateutil>=2.7.3 in /Users/polina/opt/anaconda3/lib/python3.7/site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.17.3; platform_machine != \"aarch64\" and platform_machine != \"arm64\" and python_version < \"3.10\" in /Users/polina/opt/anaconda3/lib/python3.7/site-packages (from pandas) (1.20.0)\n",
      "Requirement already satisfied, skipping upgrade: pytz>=2017.3 in /Users/polina/opt/anaconda3/lib/python3.7/site-packages (from pandas) (2019.3)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.5 in /Users/polina/opt/anaconda3/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
      "\u001b[31mERROR: sklearn-pandas 2.2.0 has requirement scikit-learn>=0.23.0, but you'll have scikit-learn 0.22.1 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: sklearn-pandas 2.2.0 has requirement scipy>=1.5.1, but you'll have scipy 1.4.1 which is incompatible.\u001b[0m\n",
      "Installing collected packages: pandas\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 1.3.1\n",
      "    Uninstalling pandas-1.3.1:\n",
      "      Successfully uninstalled pandas-1.3.1\n",
      "Successfully installed pandas-1.3.5\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "import_optional_dependency() got an unexpected keyword argument 'errors'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-62e0e3a786eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_notebook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mvis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgensim_models\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlda_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid2word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mvis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pyLDAvis/gensim_models.py\u001b[0m in \u001b[0;36mprepare\u001b[0;34m(topic_model, corpus, dictionary, doc_topic_dist, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \"\"\"\n\u001b[1;32m    122\u001b[0m     \u001b[0mopts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_extract_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_topic_dist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mopts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pyLDAvis/_prepare.py\u001b[0m in \u001b[0;36mprepare\u001b[0;34m(topic_term_dists, doc_topic_dists, doc_lengths, vocab, term_frequency, R, lambda_step, mds, n_jobs, plot_opts, sort_topics, start_index)\u001b[0m\n\u001b[1;32m    413\u001b[0m     \u001b[0mdoc_lengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_series_with_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'doc_length'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m     \u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_series_with_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'vocab'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m     \u001b[0m_input_validate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic_term_dists\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_topic_dists\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterm_frequency\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m     \u001b[0mR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pyLDAvis/_prepare.py\u001b[0m in \u001b[0;36m_input_validate\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_input_validate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_input_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValidationError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m' * '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pyLDAvis/_prepare.py\u001b[0m in \u001b[0;36m_input_check\u001b[0;34m(topic_term_dists, doc_topic_dists, doc_lengths, vocab, term_frequency)\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0merr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0m__num_dist_rows__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic_term_dists\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mttds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0merr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Not all rows (distributions) in topic_term_dists sum to 1.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pyLDAvis/_prepare.py\u001b[0m in \u001b[0;36m__num_dist_rows__\u001b[0;34m(array, ndigits)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m__num_dist_rows__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndigits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0.999\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/core/ops/common.py\u001b[0m in \u001b[0;36mnew_method\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     63\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mother\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitem_from_zerodim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/core/arraylike.py\u001b[0m in \u001b[0;36m__lt__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__ne__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cmp_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mne\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0munpack_zerodim_and_defer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"__lt__\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__lt__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_cmp_method\u001b[0;34m(self, other, op)\u001b[0m\n\u001b[1;32m   4976\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4977\u001b[0m         \u001b[0mReturns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4978\u001b[0;31m         \u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4979\u001b[0m         \u001b[0mSeries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4980\u001b[0m             \u001b[0mSeries\u001b[0m \u001b[0mof\u001b[0m \u001b[0mbooleans\u001b[0m \u001b[0mindicating\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0meach\u001b[0m \u001b[0melement\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/core/ops/array_ops.py\u001b[0m in \u001b[0;36mcomparison_op\u001b[0;34m(left, right, op)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0mlvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_wrapped_if_datetimelike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0mrvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_wrapped_if_datetimelike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m     \u001b[0mrvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem_from_zerodim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/core/ops/array_ops.py\u001b[0m in \u001b[0;36m_na_arithmetic_op\u001b[0;34m(left, right, op, is_cmp)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_na_arithmetic_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_cmp\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m     \"\"\"\n\u001b[0m\u001b[1;32m    140\u001b[0m     \u001b[0mReturn\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mresult\u001b[0m \u001b[0mof\u001b[0m \u001b[0mevaluating\u001b[0m \u001b[0mop\u001b[0m \u001b[0mon\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mpassed\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/core/computation/expressions.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_typing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFuncType\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomputation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNUMEXPR_INSTALLED\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mroperator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/core/computation/check.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optional\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimport_optional_dependency\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mne\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimport_optional_dependency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"numexpr\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"warn\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mNUMEXPR_INSTALLED\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mne\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mNUMEXPR_INSTALLED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: import_optional_dependency() got an unexpected keyword argument 'errors'"
     ]
    }
   ],
   "source": [
    "# Visualize the topics\n",
    "!pip install pandas --upgrade\n",
    "import pyLDAvis\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim_models.prepare(lda_model, corpus, id2word)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "whPN7jBLKIBc",
    "outputId": "c8131194-9861-40ba-a63f-4a9188e99e65"
   },
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_texts\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "import gensim\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "common_dictionary = Dictionary(common_texts) # create corpus\n",
    "common_corpus = [common_dictionary.doc2bow(text) for text in common_texts]\n",
    "\n",
    "lda = gensim.models.LdaModel(common_corpus, num_topics=10) # train model on corpus\n",
    "for t in range(lda.num_topics):\n",
    "    plt.figure()\n",
    "    plt.imshow(WordCloud().fit_words(dict(lda_model.show_topic(t, 200))))\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Topic #\" + str(t))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 367
    },
    "id": "QrYmz7wNKrHw",
    "outputId": "57ea17e8-cc9c-43cc-d1d0-2e7e96169a9b"
   },
   "outputs": [],
   "source": [
    "# lda is assumed to be the variable holding the LdaModel object\n",
    "import matplotlib.pyplot as plt\n",
    "for t in range(lda.num_topics):\n",
    "   plt.figure()\n",
    "plt.imshow(WordCloud().fit_words(dict(lda_model.show_topic(t, 200))))\n",
    "#if we use \"lda\" instead if \"lda_model\", numbers (id of a token would appear)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Topic #\" + str(t))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 378
    },
    "id": "WGlzGHZyI9Hr",
    "outputId": "c540be90-4d63-4968-89bd-d293c39aca8e"
   },
   "outputs": [],
   "source": [
    "!pip install pandas --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pc3UDWenLQbv"
   },
   "source": [
    "You can get the topn words from an LDA model using Gensim's built-in method show_topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 232
    },
    "id": "Ej-Xh0YM_gGD",
    "outputId": "ca038761-dced-4bd7-bc32-23a0a8f16869"
   },
   "outputs": [],
   "source": [
    "lda = models.LdaModel.load('lda.model')\n",
    "\n",
    "for i in range(0, lda.num_topics):\n",
    "    with open('output_file.txt', 'w') as outfile:\n",
    "        outfile.write('{}\\n'.format('Topic #' + str(i + 1) + ': '))\n",
    "        for word, prob in lda.show_topic(i, topn=20):\n",
    "            outfile.write('{}\\n'.format(word.encode('utf-8')))\n",
    "        outfile.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 341
    },
    "id": "-ea4oVj1L4DP",
    "outputId": "b412697c-14b4-42d0-9f85-de653c2b6c37"
   },
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim_models as gensimvis\n",
    "\n",
    "gensimvis.prepare(lda_model, corpus, id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s20THKUxNPSP",
    "outputId": "e91ee47e-a0e6-4580-b531-133ea3604033"
   },
   "outputs": [],
   "source": [
    "lda_model.show_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K44M9ughMduc",
    "outputId": "f6d9518a-cdf8-47a8-d764-cc622cda995b"
   },
   "outputs": [],
   "source": [
    "lda.show_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark\n",
    "\n",
    "# python, ipython, packages, and machine characteristics\n",
    "%watermark -v -m -p wget,pandas,numpy,geopy,altair,vega,vega_datasets,watermark \n",
    "\n",
    "# date\n",
    "print (\" \")\n",
    "%watermark -u -n -t -z "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "EDA_TFM.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
